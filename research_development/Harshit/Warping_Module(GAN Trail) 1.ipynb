{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Warping_Module.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arjunparmar/VIRTUON/blob/main/Harshit/Warping_Module(GAN%20Trail)%201.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1MZFd1fEwUo",
        "outputId": "b2f61dc4-599a-4a6c-83ed-d4d7f98c3002"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gniOyhXl-nCW",
        "outputId": "7d33821d-bbef-47d4-9cd1-be207836ab3f"
      },
      "source": [
        "!pip install --upgrade neural_structured_learning"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: neural_structured_learning in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from neural_structured_learning) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: attrs in /usr/local/lib/python3.6/dist-packages (from neural_structured_learning) (20.3.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from neural_structured_learning) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.6/dist-packages (from neural_structured_learning) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->neural_structured_learning) (1.19.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1F0RBiQrb-8"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras.layers import Conv2D, concatenate, Dropout,MaxPool2D, MaxPooling2D, Conv2DTranspose, Activation, BatchNormalization,UpSampling2D, Add\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "import neural_structured_learning as nsl\n",
        "\n",
        "import glob\n",
        "import imageio\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "\n",
        "from IPython import display"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7J_YG27s5Br"
      },
      "source": [
        "im_height, im_width, im_channels = (128,128,3)\n",
        "n_classes = 20\n",
        "batch = 32"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_N39b2jhvvF7"
      },
      "source": [
        "#Build the model\n",
        "def conv_2d_block (x,n_filters,k_size = 3,batchnorm = False):\n",
        "#1st layer\n",
        "  x= Conv2D(filters=n_filters,kernel_size=(k_size,k_size) ,padding='same', kernel_initializer = 'he_normal')(x)\n",
        "  if batchnorm:\n",
        "    x= BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "#2nd Layer\n",
        "  x= Conv2D(filters=n_filters,kernel_size=(k_size,k_size) ,padding='same', kernel_initializer = 'he_normal')(x)\n",
        "  if batchnorm:\n",
        "    x= BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "\n",
        "  return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVaPXi5tsl3h"
      },
      "source": [
        "def clothing_encoder(inputs, n_filters=16,conv_k_size=3,pool_size=2,batchnorm=True,dropout=.2):\n",
        "\n",
        "  c1 = conv_2d_block(inputs,n_filters*1,conv_k_size,batchnorm)  \n",
        "  p1 = MaxPool2D(pool_size=(pool_size,pool_size))(c1)\n",
        "  p1 = Dropout(dropout)(p1)\n",
        "  \n",
        "  c2 = conv_2d_block(p1,n_filters*2,conv_k_size,batchnorm)  \n",
        "  p2 = MaxPool2D(pool_size=(pool_size,pool_size))(c2)\n",
        "  p2 = Dropout(dropout)(p2)\n",
        "\n",
        "  c3 = conv_2d_block(p2,n_filters*4,conv_k_size,batchnorm)  \n",
        "  p3 = MaxPool2D(pool_size=(pool_size,pool_size))(c3)\n",
        "  p3 = Dropout(dropout)(p3)\n",
        "\n",
        "  c4 = conv_2d_block(p3,n_filters*8,conv_k_size,batchnorm)  \n",
        "  p4 = MaxPool2D(pool_size=(pool_size,pool_size))(c4)\n",
        "  p4 = Dropout(dropout)(p4)\n",
        "\n",
        "  c5 = conv_2d_block(p4,n_filters*16,conv_k_size,batchnorm)  \n",
        "  p5 = MaxPool2D(pool_size=(pool_size,pool_size))(c5)\n",
        "  p5 = Dropout(dropout)(p5)\n",
        "\n",
        "  c6 = conv_2d_block(p5,n_filters*32,conv_k_size,batchnorm)  \n",
        "  p6 = MaxPool2D(pool_size=(pool_size,pool_size))(c6)\n",
        "  p6 = Dropout(dropout)(p6)\n",
        "\n",
        "  c7 = conv_2d_block(p6,n_filters*64,conv_k_size,batchnorm)\n",
        " \n",
        "\n",
        "  a =  tf.keras.layers.UpSampling2D(size=(8, 8), interpolation=\"nearest\")(c7)\n",
        "\n",
        "  c8 = conv_2d_block(a,n_filters*32,conv_k_size,batchnorm)  \n",
        "\n",
        "\n",
        "  return c8"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qLVFA2jteK_"
      },
      "source": [
        "def pose_encoder(inputs, n_filters=16,conv_k_size=3,pool_size=2,batchnorm=True,dropout=.2):\n",
        "\n",
        "  c01 = conv_2d_block(inputs,n_filters*1,conv_k_size,batchnorm)  \n",
        "  p01 = MaxPool2D(pool_size=(pool_size,pool_size))(c01)\n",
        "  p01 = Dropout(dropout)(p01)\n",
        "  \n",
        "  c02 = conv_2d_block(p01,n_filters*2,conv_k_size,batchnorm)  \n",
        "  p02 = MaxPool2D(pool_size=(pool_size,pool_size))(c02)\n",
        "  p02 = Dropout(dropout)(p02)\n",
        "\n",
        "  c03 = conv_2d_block(p02,n_filters*4,conv_k_size,batchnorm)  \n",
        "  p03 = MaxPool2D(pool_size=(pool_size,pool_size))(c03)\n",
        "  p03 = Dropout(dropout)(p03)\n",
        "\n",
        "  c04 = conv_2d_block(p03,n_filters*8,conv_k_size,batchnorm)  \n",
        " \n",
        "\n",
        "  c05 = conv_2d_block(c04,n_filters*16,conv_k_size,batchnorm)  \n",
        "  \n",
        "\n",
        "  c06 = conv_2d_block(c05,n_filters*32,conv_k_size,batchnorm)\n",
        " \n",
        "  return c06"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jidBA0it83E"
      },
      "source": [
        "def res_identity(x, filters): \n",
        "  #renet block where dimension doesnot change.\n",
        "  #The skip connection is just simple identity conncection\n",
        "  #we will have 3 blocks and then input will be added\n",
        "\n",
        "  x_skip = x # this will be used for addition with the residual block \n",
        "  f1, f2 = filters\n",
        "\n",
        "  #first block \n",
        "  x = Conv2D(f1, kernel_size=(1, 1), strides=(1, 1), padding='valid')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "\n",
        "  #second block # bottleneck (but size kept same with padding)\n",
        "  x = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "\n",
        "  # third block activation used after adding the input\n",
        "  x = Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  # x = Activation('relu')(x)\n",
        "  \n",
        "  # add the input \n",
        "  x = Add()([x, x_skip])\n",
        "  x = Activation('relu')(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "def res_conv(x, s, filters):\n",
        "  '''\n",
        "  here the input size changes''' \n",
        "  x_skip = x\n",
        "  f1, f2 = filters\n",
        "\n",
        "  # first block\n",
        "  x = Conv2D(f1, kernel_size=(1, 1), strides=(s, s), padding='valid')(x)\n",
        "  # when s = 2 then it is like downsizing the feature map\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "\n",
        "  # second block\n",
        "  x = Conv2D(f1, kernel_size=(3, 3), strides=(1, 1), padding='same')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "\n",
        "  #third block\n",
        "  x = Conv2D(f2, kernel_size=(1, 1), strides=(1, 1), padding='valid')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  # shortcut \n",
        "  x_skip = Conv2D(f2, kernel_size=(1, 1), strides=(s, s), padding='valid')(x_skip)\n",
        "  x_skip = BatchNormalization()(x_skip)\n",
        "\n",
        "  # add \n",
        "  x = Add()([x, x_skip])\n",
        "  x = Activation('relu')(x)\n",
        "\n",
        "  return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pRVVzUHuOfF"
      },
      "source": [
        "def res_block(x, s, filters):\n",
        "\n",
        "  # input_im = Input(shape=(x.shape[1], x.shape[2], x.shape[3])) # cifar 10 images size\n",
        "  # x = ZeroPadding2D(padding=(3, 3))(input_im)\n",
        "\n",
        "  # # 1st stage\n",
        "  # # here we perform maxpooling, see the figure above\n",
        "\n",
        "  # x = BatchNormalization()(x)\n",
        "  # x = Activation(activations.relu)(x)\n",
        "\n",
        "\n",
        "  #2nd stage \n",
        "  # frm here on only conv block and identity block, no pooling\n",
        "  x = res_conv(x, s, filters)\n",
        "  x = res_identity(x, filters=(64, 512))\n",
        "  x = res_identity(x, filters=(64, 512))\n",
        "\n",
        "  # 3rd stage\n",
        "\n",
        "  x = res_identity(x, filters=(128, 512))\n",
        "  x = res_identity(x, filters=(128, 512))\n",
        "  x = res_identity(x, filters=(128, 512))\n",
        "  x = res_identity(x, filters=(128, 512))\n",
        "  x = res_identity(x, filters=(128, 512))\n",
        "  x = res_identity(x, filters=(128, 512))\n",
        "\n",
        "\n",
        "  # 4th stage\n",
        "\n",
        "  x = res_identity(x, filters=(256, 512))\n",
        "  x = res_identity(x, filters=(256, 512))\n",
        "  x = res_identity(x, filters=(256, 512))\n",
        "  x = res_identity(x, filters=(256, 512))\n",
        "  x = res_identity(x, filters=(256, 512))\n",
        "\n",
        "  # 5th stage\n",
        "\n",
        "  x = res_identity(x, filters=(512, 512))\n",
        "  x = res_identity(x, filters=(512, 512))\n",
        "\n",
        "  # ends with average pooling and dense connection\n",
        "\n",
        "  # x = AveragePooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "  # x = Flatten()(x)\n",
        "  # x = Dense(len(class_types), activation='softmax', kernel_initializer='he_normal')(x) #multi-class\n",
        "\n",
        "  return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbr9wvAoumqQ"
      },
      "source": [
        "#cloth decoder \n",
        "def clothing_decoder(inputs,n_filters=16,conv_k_size=3,pool_size=2,batchnorm=True,dropout=.2):\n",
        "  u6 = Conv2DTranspose(filters=n_filters *16 ,kernel_size=(3,3), strides=(2,2),padding='same')(inputs)\n",
        "  u6 = Dropout(dropout)(u6)\n",
        "  c7 = conv_2d_block(u6,n_filters *16 , conv_k_size,batchnorm)\n",
        "\n",
        "  u7 = Conv2DTranspose(filters=n_filters *8,kernel_size=(3,3), strides=(2,2),padding='same')(c7)\n",
        "  u7 = Dropout(dropout)(u7)\n",
        "  c8 = conv_2d_block(u7,n_filters *8 , conv_k_size,batchnorm)\n",
        "\n",
        "  u8 = Conv2DTranspose(filters=n_filters *4,kernel_size=(3,3), strides=(2,2),padding='same')(c8)\n",
        "  u8 = Dropout(dropout)(u8)\n",
        "  c9 = conv_2d_block(u8,n_filters *4, conv_k_size,batchnorm)\n",
        "\n",
        "  c10 = Conv2D(filters=n_classes, kernel_size=(1,1),activation='softmax')(c9)\n",
        "  \n",
        "  return c10"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAX2Lyknrlea"
      },
      "source": [
        "def warp(cloth_input, pose_input):\n",
        "    cloth = clothing_encoder(cloth_input)\n",
        "    pose = pose_encoder(pose_input)\n",
        "    both = concatenate([cloth, pose])\n",
        "    resnet = res_block(both, 1, (64,512))\n",
        "    output = clothing_decoder(resnet)\n",
        "\n",
        "    return Model(inputs = (cloth_input, pose_input), outputs = output, name = \"Warping_Module\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVllr1HW-XlB"
      },
      "source": [
        "def dis(input, n_filters=16,k_size=3,pool_size=2,batchnorm=True,dropout=.2):\n",
        "    x = Conv2D(filters=64,kernel_size=(k_size,k_size) ,padding='same', kernel_initializer = 'he_normal')(input)\n",
        "    x = MaxPool2D(pool_size = (pool_size, pool_size))(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters=64,kernel_size=(k_size,k_size) ,padding='same', kernel_initializer = 'he_normal')(x)\n",
        "    x = MaxPool2D(pool_size = (pool_size, pool_size))(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters=64,kernel_size=(k_size,k_size) ,padding='same', kernel_initializer = 'he_normal')(x)\n",
        "    x = MaxPool2D(pool_size = (pool_size, pool_size))(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dense(64, activation = 'relu', kernel_initializer = 'he_normal')(x)\n",
        "    x = tf.keras.layers.Dense(1, activation = 'sigmoid', kernel_initializer = 'he_normal')(x)\n",
        "\n",
        "    return Model(inputs = input, outputs = x, name = 'Discriminator')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tSOIa7FCtVc"
      },
      "source": [
        "discriminator = dis(Input((im_height, im_width, n_classes)))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnFiS4IRyD-m"
      },
      "source": [
        "cloth = Input((im_height, im_width, n_classes), name = \"cloth\")\n",
        "pose = Input((im_height, im_width, im_channels), name = 'pose')\n",
        "generator = warp(cloth, pose)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5iwVobGExWE"
      },
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPp5jDl0JJta"
      },
      "source": [
        "EPOCHS = 50\n",
        "noise_dim = 100\n",
        "num_examples_to_generate = 16\n",
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = batch\n",
        "\n",
        "# We will reuse this seed overtime (so it's easier)\n",
        "# to visualize progress in the animated GIF)\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BISB5X8oGEmA"
      },
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "\n",
        "    # Produce images for the GIF as we go\n",
        "    # display.clear_output(wait=True)\n",
        "    # generate_and_save_images(generator,\n",
        "    #                          epoch + 1,\n",
        "    #                          seed)\n",
        "\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "  # Generate after the final epoch\n",
        "#   display.clear_output(wait=True)\n",
        "#   generate_and_save_images(generator,\n",
        "#                            epochs,\n",
        "#                            seed)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOODtSUvFdN5"
      },
      "source": [
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, im_height, im_width, n_classes])\n",
        "    noise_1 = tf.random.normal([BATCH_SIZE, im_height, im_width, im_channels])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images =generator((noise, noise_1), training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss,generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator,generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DunB3pk0GWR5"
      },
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1273jPyunU8"
      },
      "source": [
        "x_train_pos = np.load(\"/content/drive/Shareddrives/Virtuon/Clothing Coparsing/dataset/pos_train.npy\")\r\n",
        "x_train_seg = np.load(\"/content/drive/Shareddrives/Virtuon/Clothing Coparsing/dataset/seg_train_argmax.npy\")\r\n",
        "x_valid_pos = np.load(\"/content/drive/Shareddrives/Virtuon/Clothing Coparsing/dataset/x_test.npy\")\r\n",
        "x_valid_seg = np.load(\"/content/drive/Shareddrives/Virtuon/Clothing Coparsing/dataset/seg_test_argmax.npy\")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8uQHjEovwT6"
      },
      "source": [
        "x_train_seg_1 = tf.one_hot(x_train_seg, depth = n_classes)\n",
        "x_valid_seg_1 = tf.one_hot(x_valid_seg, depth = n_classes)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFMpXWI11Ppm"
      },
      "source": [
        "IMG_SIZE = 128\r\n",
        "\r\n",
        "data_augmentation = tf.keras.Sequential([\r\n",
        "  tf.keras.layers.experimental.preprocessing.Resizing(IMG_SIZE, IMG_SIZE),\r\n",
        "  tf.keras.layers.experimental.preprocessing.Rescaling(1./255),\r\n",
        "  tf.keras.layers.experimental.preprocessing.RandomFlip(\"vertical\"),\r\n",
        "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\r\n",
        "  tf.keras.layers.experimental.preprocessing.RandomZoom(0.2)\r\n",
        "#   tf.keras.layers.experimental.preprocessing.RandomCrop(0.2,0.2)\r\n",
        "])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xERBJ6aqyutz"
      },
      "source": [
        "def preprocessing(pos, seg):\r\n",
        "    seg_1 = data_augmentation(seg)\r\n",
        "    pos = tf.image.resize(pos, [im_height, im_width])\r\n",
        "    seg = tf.image.resize(seg, [im_height, im_width])\r\n",
        "    return (seg_1, pos), seg"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRV4hSFQxrmj"
      },
      "source": [
        "def generator_train():\r\n",
        "    while True:\r\n",
        "        for i in range(0,len(x_train_pos),batch):\r\n",
        "            if (i + batch < len(x_train_pos)):\r\n",
        "                pos = x_train_pos[i:i+batch]\r\n",
        "                seg = x_train_seg_1[i:i+batch]\r\n",
        "                yield preprocessing(pos, seg)\r\n",
        "            else:\r\n",
        "                last = len(x_train_pos)\r\n",
        "                pos = x_train_pos[i:last]\r\n",
        "                seg = x_train_seg_1[i:last]\r\n",
        "                yield preprocessing(pos, seg)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FfewokwpkHL"
      },
      "source": [
        "def generator_valid():\n",
        "    while True:\n",
        "        for i in range(0,len(x_valid_pos),batch):\n",
        "            if (i + batch < len(x_valid_pos)):\n",
        "                pos = x_valid_pos[i:i+batch]\n",
        "                seg = x_valid_seg_1[i:i+batch]\n",
        "                yield preprocessing(pos, seg)\n",
        "            else:\n",
        "                last = len(x_valid_pos)\n",
        "                pos = x_valid_pos[i:last]\n",
        "                seg = x_valid_seg_1[i:last]\n",
        "                yield preprocessing(pos, seg)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RooaA6_Arv5d",
        "outputId": "9dd5a7c3-7611-46ce-d7f2-38440644fc52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "x_train_seg_aug = data_augmentation(x_train_seg_1[0:batch])\n",
        "train( tf.reshape(x_train_seg_aug, (1,32,128,128,20)), 1000)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time for epoch 1 is 0.4660158157348633 sec\n",
            "Time for epoch 2 is 0.5252277851104736 sec\n",
            "Time for epoch 3 is 0.5299828052520752 sec\n",
            "Time for epoch 4 is 0.5245494842529297 sec\n",
            "Time for epoch 5 is 0.5277807712554932 sec\n",
            "Time for epoch 6 is 0.5262613296508789 sec\n",
            "Time for epoch 7 is 0.5305254459381104 sec\n",
            "Time for epoch 8 is 0.527226448059082 sec\n",
            "Time for epoch 9 is 0.5285890102386475 sec\n",
            "Time for epoch 10 is 0.5282011032104492 sec\n",
            "Time for epoch 11 is 0.5311031341552734 sec\n",
            "Time for epoch 12 is 0.5289947986602783 sec\n",
            "Time for epoch 13 is 0.528656005859375 sec\n",
            "Time for epoch 14 is 0.526566743850708 sec\n",
            "Time for epoch 15 is 0.530325174331665 sec\n",
            "Time for epoch 16 is 0.5279712677001953 sec\n",
            "Time for epoch 17 is 0.5310816764831543 sec\n",
            "Time for epoch 18 is 0.5300207138061523 sec\n",
            "Time for epoch 19 is 0.5329864025115967 sec\n",
            "Time for epoch 20 is 0.5281214714050293 sec\n",
            "Time for epoch 21 is 0.5311717987060547 sec\n",
            "Time for epoch 22 is 0.5298583507537842 sec\n",
            "Time for epoch 23 is 0.5320594310760498 sec\n",
            "Time for epoch 24 is 0.5297589302062988 sec\n",
            "Time for epoch 25 is 0.5322134494781494 sec\n",
            "Time for epoch 26 is 0.5288724899291992 sec\n",
            "Time for epoch 27 is 0.5327286720275879 sec\n",
            "Time for epoch 28 is 0.5310006141662598 sec\n",
            "Time for epoch 29 is 0.5322315692901611 sec\n",
            "Time for epoch 30 is 0.5304811000823975 sec\n",
            "Time for epoch 31 is 0.5329482555389404 sec\n",
            "Time for epoch 32 is 0.5306282043457031 sec\n",
            "Time for epoch 33 is 0.5339229106903076 sec\n",
            "Time for epoch 34 is 0.5306596755981445 sec\n",
            "Time for epoch 35 is 0.5350806713104248 sec\n",
            "Time for epoch 36 is 0.531275749206543 sec\n",
            "Time for epoch 37 is 0.532768726348877 sec\n",
            "Time for epoch 38 is 0.5317082405090332 sec\n",
            "Time for epoch 39 is 0.5338799953460693 sec\n",
            "Time for epoch 40 is 0.5315699577331543 sec\n",
            "Time for epoch 41 is 0.5333836078643799 sec\n",
            "Time for epoch 42 is 0.5343408584594727 sec\n",
            "Time for epoch 43 is 0.5349123477935791 sec\n",
            "Time for epoch 44 is 0.5357820987701416 sec\n",
            "Time for epoch 45 is 0.536949634552002 sec\n",
            "Time for epoch 46 is 0.5351035594940186 sec\n",
            "Time for epoch 47 is 0.5386917591094971 sec\n",
            "Time for epoch 48 is 0.5342261791229248 sec\n",
            "Time for epoch 49 is 0.5372281074523926 sec\n",
            "Time for epoch 50 is 0.5338339805603027 sec\n",
            "Time for epoch 51 is 0.5394747257232666 sec\n",
            "Time for epoch 52 is 0.5333034992218018 sec\n",
            "Time for epoch 53 is 0.5395419597625732 sec\n",
            "Time for epoch 54 is 0.5330712795257568 sec\n",
            "Time for epoch 55 is 0.5402498245239258 sec\n",
            "Time for epoch 56 is 0.5331132411956787 sec\n",
            "Time for epoch 57 is 0.542203426361084 sec\n",
            "Time for epoch 58 is 0.5340096950531006 sec\n",
            "Time for epoch 59 is 0.5413131713867188 sec\n",
            "Time for epoch 60 is 0.5357530117034912 sec\n",
            "Time for epoch 61 is 0.5406382083892822 sec\n",
            "Time for epoch 62 is 0.5359230041503906 sec\n",
            "Time for epoch 63 is 0.538428544998169 sec\n",
            "Time for epoch 64 is 0.5398252010345459 sec\n",
            "Time for epoch 65 is 0.5381765365600586 sec\n",
            "Time for epoch 66 is 0.5372951030731201 sec\n",
            "Time for epoch 67 is 0.5422163009643555 sec\n",
            "Time for epoch 68 is 0.5372254848480225 sec\n",
            "Time for epoch 69 is 0.5411596298217773 sec\n",
            "Time for epoch 70 is 0.539114236831665 sec\n",
            "Time for epoch 71 is 0.5371243953704834 sec\n",
            "Time for epoch 72 is 0.5386009216308594 sec\n",
            "Time for epoch 73 is 0.5362131595611572 sec\n",
            "Time for epoch 74 is 0.5396416187286377 sec\n",
            "Time for epoch 75 is 0.5371983051300049 sec\n",
            "Time for epoch 76 is 0.5393564701080322 sec\n",
            "Time for epoch 77 is 0.538978099822998 sec\n",
            "Time for epoch 78 is 0.5374841690063477 sec\n",
            "Time for epoch 79 is 0.539722204208374 sec\n",
            "Time for epoch 80 is 0.5403246879577637 sec\n",
            "Time for epoch 81 is 0.5381474494934082 sec\n",
            "Time for epoch 82 is 0.541339635848999 sec\n",
            "Time for epoch 83 is 0.5402281284332275 sec\n",
            "Time for epoch 84 is 0.5399899482727051 sec\n",
            "Time for epoch 85 is 0.5410254001617432 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-1da7eddf3c06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx_train_seg_aug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_augmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_seg_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_seg_aug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-ec58b6d2e831>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, epochs)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimage_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m       \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Produce images for the GIF as we go\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPyTZHrqHArX"
      },
      "source": [
        "result = generator.predict(( tf.image.resize(x_train_seg_1[11:12], [im_height, im_width]), tf.image.resize(x_train_pos[11:12], [im_height, im_width]) ))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91pzCAqsPKfS",
        "outputId": "484e3377-1417-4094-dd66-cb83705b1e51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "source": [
        "plt.imshow()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f1529b90630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tmgehoJPNCF",
        "outputId": "6db9f77a-8cd1-4965-9b3a-ef175356cce7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.unique(tf.math.argmax(result[0], axis = -1))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    }
  ]
}