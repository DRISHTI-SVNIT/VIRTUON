{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TOM_DataPreprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arjunparmar/VIRTUON/blob/main/Harshit/Pytorch/v4_TOM_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyFS5LZJJdI_",
        "outputId": "98cfb44d-d0b8-42ec-c900-9ef1b5a87aa6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRjZxl_q1QtY"
      },
      "source": [
        "!cp /content/drive/Shareddrives/Virtuon/Pytorch/cp-vton-plus.zip /content/"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ypd6IEiKNybZ"
      },
      "source": [
        "!unzip -qq cp-vton-plus.zip -d /content/"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnK4vhBt-cuM",
        "outputId": "09a38764-1fc7-4fd8-d02f-4fcf7c443b51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!unzip tax.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  tax.zip\n",
            "   creating: content/test/\n",
            "   creating: content/test/test/\n",
            "   creating: content/test/test/image/\n",
            "  inflating: content/test/test/image/demo.jpeg  \n",
            "   creating: content/test/test/cloth/\n",
            "  inflating: content/test/test/cloth/000010_1.jpg  \n",
            "   creating: content/test/test/warp_mask/\n",
            "  inflating: content/test/test/warp_mask/demo.jpeg  \n",
            "   creating: content/test/test/result_dir/\n",
            "  inflating: content/test/test/result_dir/demo.jpeg  \n",
            "   creating: content/test/test/pose/\n",
            "  inflating: content/test/test/pose/demo_keypoints.json  \n",
            "   creating: content/test/test/image-parse-new/\n",
            "  inflating: content/test/test/image-parse-new/demo.jpeg  \n",
            "   creating: content/test/test/image-parse-new/.ipynb_checkpoints/\n",
            "  inflating: content/test/test/image-parse-new/demo.png  \n",
            "   creating: content/test/test/warp_cloth/\n",
            "  inflating: content/test/test/warp_cloth/demo.jpeg  \n",
            "   creating: content/test/test/overlayed_TPS/\n",
            "  inflating: content/test/test/overlayed_TPS/demo.jpeg  \n",
            "   creating: content/test/test/warped_grid/\n",
            "   creating: content/test/test/image-parse/\n",
            "   creating: content/test/test/image-mask/\n",
            "  inflating: content/test/test/image-mask/demo.jpeg  \n",
            "   creating: content/test/test/image-mask/.ipynb_checkpoints/\n",
            "  inflating: content/test/test/image-mask/demo.png  \n",
            "   creating: content/test/test/cloth-mask/\n",
            "  inflating: content/test/test/cloth-mask/000010_1.jpg  \n",
            "   creating: content/test/test/cloth-mask/.ipynb_checkpoints/\n",
            "  inflating: content/test/test_pairs.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZbddUf0BUgd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EriiCYwlH0kT"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import os.path as osp\n",
        "import json"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_iyjB38IcVl"
      },
      "source": [
        "class CPDataset(data.Dataset):\n",
        "    def __init__(self, stage, all_root=\"cp-vton-plus\", data_path = \"data\", mode=\"train\", radius=5, img_height=256, img_width=192):\n",
        "        super(CPDataset, self).__init__()\n",
        "\n",
        "        self.root = all_root\n",
        "\n",
        "        self.data_root = osp.join(all_root,data_path)\n",
        "\n",
        "        self.datamode = mode\n",
        "\n",
        "        self.stage = stage\n",
        "\n",
        "        self.data_list = \"\".join([mode, \"_pairs.txt\"])\n",
        "\n",
        "        self.fine_height = img_height\n",
        "\n",
        "        self.fine_width = img_width\n",
        "\n",
        "        self.radius = radius\n",
        "\n",
        "        self.data_path = osp.join(all_root,data_path, mode)\n",
        "        \n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "        \n",
        "        self.transform_1 = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5), (0.5))\n",
        "        ])\n",
        "\n",
        "        self.transform_2 = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5), (0.5, 0.5))\n",
        "        ])\n",
        "\n",
        "        self.transform_3 = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "        im_names = []\n",
        "        c_names = []\n",
        "\n",
        "        with open(osp.join(self.data_root, self.data_list), 'r') as f:\n",
        "            for line in f.readlines():\n",
        "                im_name, c_name = line.strip().split()\n",
        "                im_names.append(im_name)\n",
        "                c_names.append(c_name)\n",
        "\n",
        "        self.im_names = im_names\n",
        "        self.c_names = c_names\n",
        "\n",
        "    def name(self):\n",
        "        return \"CPDataset\"\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        c_name = self.c_names[index]\n",
        "        im_name = self.im_names[index]\n",
        "        if self.stage == \"GMM\":\n",
        "            c = Image.open(osp.join(self.data_path, 'cloth', c_name))\n",
        "            cm = Image.open(osp.join(self.data_path, 'cloth-mask', c_name)).convert('L')\n",
        "        else:\n",
        "            c = Image.open(osp.join(self.data_path, 'warp-cloth', im_name))\n",
        "            cm = Image.open(osp.join(self.data_path, 'warp-mask', im_name)).convert('L')\n",
        "        \n",
        "        c = self.transform(c)\n",
        "        cm_array = np.array(cm)\n",
        "        cm_array = (cm_array >= 128).astype(np.float32)\n",
        "        cm = torch.from_numpy(cm_array)\n",
        "        cm.unsqueeze_(0)\n",
        "\n",
        "        # person image\n",
        "        im = Image.open(osp.join(self.data_path, 'image', im_name))\n",
        "        im = self.transform(im)\n",
        "\n",
        "        \n",
        "        # LIP labels\n",
        "        \n",
        "        # [(0, 0, 0),    # 0=Background\n",
        "        #  (128, 0, 0),  # 1=Hat\n",
        "        #  (255, 0, 0),  # 2=Hair\n",
        "        #  (0, 85, 0),   # 3=Glove\n",
        "        #  (170, 0, 51),  # 4=SunGlasses\n",
        "        #  (255, 85, 0),  # 5=UpperClothes\n",
        "        #  (0, 0, 85),     # 6=Dress\n",
        "        #  (0, 119, 221),  # 7=Coat\n",
        "        #  (85, 85, 0),    # 8=Socks\n",
        "        #  (0, 85, 85),    # 9=Pants\n",
        "        #  (85, 51, 0),    # 10=Jumpsuits\n",
        "        #  (52, 86, 128),  # 11=Scarf\n",
        "        #  (0, 128, 0),    # 12=Skirt\n",
        "        #  (0, 0, 255),    # 13=Face\n",
        "        #  (51, 170, 221),  # 14=LeftArm\n",
        "        #  (0, 255, 255),   # 15=RightArm\n",
        "        #  (85, 255, 170),  # 16=LeftLeg\n",
        "        #  (170, 255, 85),  # 17=RightLeg\n",
        "        #  (255, 255, 0),   # 18=LeftShoe\n",
        "        #  (255, 170, 0)    # 19=RightShoe\n",
        "        #  (170, 170, 50)   # 20=Skin/Neck/Chest (Newly added after running dataset_neck_skin_correction.py)\n",
        "        #  ]\n",
        "         \n",
        "        # load parsing image\n",
        "        parse_name = im_name.replace('.jpg', '.png')\n",
        "        im_parse = Image.open(osp.join(self.data_path, 'image-parse-new',parse_name)).convert('L')\n",
        "        parse_array = np.array(im_parse)\n",
        "\n",
        "        im_mask = Image.open(osp.join(self.data_path, 'image-mask', parse_name)).convert('L')\n",
        "        mask_array = np.array(im_mask)\n",
        "\n",
        "        parse_shape = (mask_array > 0).astype(np.float32)\n",
        "\n",
        "        if self.stage == 'GMM':\n",
        "            parse_head = (parse_array == 1).astype(np.float32) + (parse_array == 4).astype(np.float32) + (parse_array == 13).astype(np.float32)\n",
        "\n",
        "        else:\n",
        "            parse_head = (parse_array == 1).astype(np.float32) + (parse_array == 2).astype(np.float32) + (parse_array == 4).astype(np.float32) + (parse_array == 9).astype(np.float32) + (parse_array == 12).astype(np.float32) + (parse_array == 13).astype(np.float32) + (parse_array == 16).astype(np.float32) + (parse_array == 17).astype(np.float32)  \n",
        "            \n",
        "        parse_cloth = (parse_array == 5).astype(np.float32) + (parse_array == 6).astype(np.float32) + (parse_array == 7).astype(np.float32)\n",
        "\n",
        "        parse_shape_ori = Image.fromarray((parse_shape*255).astype(np.uint8))\n",
        "\n",
        "        parse_shape = parse_shape_ori.resize((self.fine_width//16, self.fine_height//16), Image.BILINEAR)\n",
        "\n",
        "        parse_shape = parse_shape.resize((self.fine_width, self.fine_height), Image.BILINEAR)\n",
        "        \n",
        "        parse_shape_ori = parse_shape_ori.resize((self.fine_width, self.fine_height), Image.BILINEAR)\n",
        "        \n",
        "        shape_ori = self.transform_1(parse_shape_ori)\n",
        "\n",
        "        shape = self.transform_1(parse_shape)\n",
        "\n",
        "        phead = torch.from_numpy(parse_head)\n",
        "\n",
        "        pcm = torch.from_numpy(parse_cloth)\n",
        "\n",
        "        # Upper Cloth\n",
        "        im_c = im*pcm + (1 - pcm)\n",
        "        im_h = im*phead + (1-phead)\n",
        "\n",
        "        # load pose points\n",
        "        pose_name = im_name.replace('.jpg', '_keypoints.json')\n",
        "        with open(osp.join(self.data_path, 'pose', pose_name), 'r') as f:\n",
        "            pose_label = json.load(f)\n",
        "            pose_data = pose_label['people'][0]['pose_keypoints']\n",
        "            pose_data = np.array(pose_data)\n",
        "            pose_data = pose_data.reshape([-1,3])\n",
        "        \n",
        "        point_num = pose_data.shape[0]\n",
        "        pose_map = torch.zeros(point_num, self.fine_height, self.fine_width)\n",
        "        \n",
        "        r = self.radius\n",
        "        \n",
        "        im_pose = Image.new('L', (self.fine_width, self.fine_height))\n",
        "        pose_draw = ImageDraw.Draw(im_pose)\n",
        "\n",
        "        for i in range(point_num):\n",
        "            one_map = Image.new('L', (self.fine_width, self.fine_height))\n",
        "            draw = ImageDraw.Draw(one_map)\n",
        "            pointx = pose_data[i, 0]\n",
        "            pointy = pose_data[i, 1]\n",
        "\n",
        "            if pointx > 1 and pointy > 1:\n",
        "                draw.rectangle((pointx - r, pointy - r, pointx + r, pointy + r), 'white', 'white')\n",
        "                pose_draw.rectangle((pointx - r, pointy - r, pointx + r, pointy + r), 'white', 'white')\n",
        "\n",
        "            one_map = self.transform_1(one_map)\n",
        "            pose_map[i] = one_map[0]\n",
        "\n",
        "        im_pose = self.transform_1(im_pose)\n",
        "\n",
        "        agnostic = torch.cat([shape, im_h, pose_map], 0)\n",
        "\n",
        "        if self.stage == 'GMM':\n",
        "            im_g = Image.open(osp.join(self.root, 'grid.png'))\n",
        "            im_g = self.transform(im_g)\n",
        "        else:\n",
        "            im_g = ''\n",
        "        \n",
        "        pcm.unsqueeze_(0)\n",
        "        \n",
        "        result = {\n",
        "            'c_name': c_name,\n",
        "            'im_name': im_name,\n",
        "            'cloth': c,\n",
        "            'cloth_mask': cm,\n",
        "            'image': im,\n",
        "            'agnostic': agnostic,\n",
        "            'parse_cloth': im_c,\n",
        "            'shape': shape,\n",
        "            'head': im_h,\n",
        "            'pose_image': im_pose,\n",
        "            # 'grid_image': im_g,\n",
        "            'parse_cloth_mask': pcm,\n",
        "            'shape_ori': shape_ori,\n",
        "        }\n",
        "\n",
        "        return result\n",
        "    def __len__(self):\n",
        "        return len(self.im_names)\n",
        "\n",
        "\n",
        "class CPDataLoader(object):\n",
        "    def __init__(self, dataset, shuffle=True, batch=4, workers=4):\n",
        "        super(CPDataLoader, self).__init__()\n",
        "\n",
        "        if shuffle:\n",
        "            train_sampler = torch.utils.data.sampler.RandomSampler(dataset)\n",
        "        else:\n",
        "            train_sampler = None\n",
        "        \n",
        "        self.data_loader = torch.utils.data.DataLoader(\n",
        "            dataset, batch_size=batch, shuffle=(train_sampler is None),\n",
        "            num_workers=workers, pin_memory=True, sampler=train_sampler\n",
        "        )\n",
        "        self.dataset = dataset\n",
        "        self.data_iter = self.data_loader.__iter__()\n",
        "\n",
        "    def next_batch(self):\n",
        "        try:\n",
        "            batch = self.data_iter.__next__()\n",
        "        except StopIteration:\n",
        "            self.data_iter = self.data_loader.__iter__()\n",
        "            batch = self.data_iter.__next__()\n",
        "        \n",
        "        return batch\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpl14krN-zCh"
      },
      "source": [
        "class Vgg19(nn.Module):\r\n",
        "    def __init__(self, requires_grad=False):\r\n",
        "        super(Vgg19, self).__init__()\r\n",
        "        vgg_pretrained_features = models.vgg19(pretrained=True).features\r\n",
        "        self.slice1 = torch.nn.Sequential()\r\n",
        "        self.slice2 = torch.nn.Sequential()\r\n",
        "        self.slice3 = torch.nn.Sequential()\r\n",
        "        self.slice4 = torch.nn.Sequential()\r\n",
        "        self.slice5 = torch.nn.Sequential()\r\n",
        "        for x in range(2):\r\n",
        "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\r\n",
        "        for x in range(2, 7):\r\n",
        "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\r\n",
        "        for x in range(7, 12):\r\n",
        "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\r\n",
        "        for x in range(12, 21):\r\n",
        "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\r\n",
        "        for x in range(21, 30):\r\n",
        "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\r\n",
        "        if not requires_grad:\r\n",
        "            for param in self.parameters():\r\n",
        "                param.requires_grad = False\r\n",
        "\r\n",
        "    def forward(self, X):\r\n",
        "        h_relu1 = self.slice1(X)\r\n",
        "        h_relu2 = self.slice2(h_relu1)\r\n",
        "        h_relu3 = self.slice3(h_relu2)\r\n",
        "        h_relu4 = self.slice4(h_relu3)\r\n",
        "        h_relu5 = self.slice5(h_relu4)\r\n",
        "        out = [h_relu1, h_relu2, h_relu3, h_relu4, h_relu5]\r\n",
        "        return out\r\n",
        "\r\n",
        "\r\n",
        "class VGGLoss(nn.Module):\r\n",
        "    def __init__(self, layids=None):\r\n",
        "        super(VGGLoss, self).__init__()\r\n",
        "        self.vgg = Vgg19()\r\n",
        "        self.vgg.cuda()\r\n",
        "        self.criterion = nn.L1Loss()\r\n",
        "        self.weights = [1.0/32, 1.0/16, 1.0/8, 1.0/4, 1.0]\r\n",
        "        self.layids = layids\r\n",
        "\r\n",
        "    def forward(self, x, y):\r\n",
        "        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\r\n",
        "        loss = 0\r\n",
        "        if self.layids is None:\r\n",
        "            self.layids = list(range(len(x_vgg)))\r\n",
        "        for i in self.layids:\r\n",
        "            loss += self.weights[i] * \\\r\n",
        "                self.criterion(x_vgg[i], y_vgg[i].detach())\r\n",
        "        return loss\r\n",
        "\r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkZ2YFwkD927"
      },
      "source": [
        "class UnetGenerator(nn.Module):\r\n",
        "    def __init__(self, input_nc, output_nc, num_downs, ngf=64,\r\n",
        "                 norm_layer=nn.BatchNorm2d, use_dropout=False):\r\n",
        "        super(UnetGenerator, self).__init__()\r\n",
        "        # construct unet structure\r\n",
        "        unet_block = UnetSkipConnectionBlock(\r\n",
        "            ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True)\r\n",
        "        for i in range(num_downs - 5):\r\n",
        "            unet_block = UnetSkipConnectionBlock(\r\n",
        "                ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\r\n",
        "        unet_block = UnetSkipConnectionBlock(\r\n",
        "            ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\r\n",
        "        unet_block = UnetSkipConnectionBlock(\r\n",
        "            ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\r\n",
        "        unet_block = UnetSkipConnectionBlock(\r\n",
        "            ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\r\n",
        "        unet_block = UnetSkipConnectionBlock(\r\n",
        "            output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)\r\n",
        "\r\n",
        "        self.model = unet_block\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        return self.model(input)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPmnaCno-84_"
      },
      "source": [
        "class UnetSkipConnectionBlock(nn.Module):\r\n",
        "    def __init__(self, outer_nc, inner_nc, input_nc=None,\r\n",
        "                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\r\n",
        "        super(UnetSkipConnectionBlock, self).__init__()\r\n",
        "        self.outermost = outermost\r\n",
        "        use_bias = norm_layer == nn.InstanceNorm2d\r\n",
        "\r\n",
        "        if input_nc is None:\r\n",
        "            input_nc = outer_nc\r\n",
        "        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\r\n",
        "                             stride=2, padding=1, bias=use_bias)\r\n",
        "        downrelu = nn.LeakyReLU(0.2, True)\r\n",
        "        downnorm = norm_layer(inner_nc)\r\n",
        "        uprelu = nn.ReLU(True)\r\n",
        "        upnorm = norm_layer(outer_nc)\r\n",
        "\r\n",
        "        if outermost:\r\n",
        "            upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\r\n",
        "            upconv = nn.Conv2d(inner_nc * 2, outer_nc,\r\n",
        "                               kernel_size=3, stride=1, padding=1, bias=use_bias)\r\n",
        "            down = [downconv]\r\n",
        "            up = [uprelu, upsample, upconv, upnorm]\r\n",
        "            model = down + [submodule] + up\r\n",
        "        elif innermost:\r\n",
        "            upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\r\n",
        "            upconv = nn.Conv2d(inner_nc, outer_nc, kernel_size=3,\r\n",
        "                               stride=1, padding=1, bias=use_bias)\r\n",
        "            down = [downrelu, downconv]\r\n",
        "            up = [uprelu, upsample, upconv, upnorm]\r\n",
        "            model = down + up\r\n",
        "        else:\r\n",
        "            upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\r\n",
        "            upconv = nn.Conv2d(inner_nc*2, outer_nc, kernel_size=3,\r\n",
        "                               stride=1, padding=1, bias=use_bias)\r\n",
        "            down = [downrelu, downconv, downnorm]\r\n",
        "            up = [uprelu, upsample, upconv, upnorm]\r\n",
        "\r\n",
        "            if use_dropout:\r\n",
        "                model = down + [submodule] + up + [nn.Dropout(0.5)]\r\n",
        "            else:\r\n",
        "                model = down + [submodule] + up\r\n",
        "\r\n",
        "        self.model = nn.Sequential(*model)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        if self.outermost:\r\n",
        "            return self.model(x)\r\n",
        "        else:\r\n",
        "            return torch.cat([x, self.model(x)], 1)\r\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-91U5eSMUMA"
      },
      "source": [
        "import torch\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "def tensor_for_board(img_tensor):\r\n",
        "    # map into [0,1]\r\n",
        "    tensor = (img_tensor.clone()+1) * 0.5\r\n",
        "    tensor.cpu().clamp(0, 1)\r\n",
        "\r\n",
        "    if tensor.size(1) == 1:\r\n",
        "        tensor = tensor.repeat(1, 3, 1, 1)\r\n",
        "\r\n",
        "    return tensor\r\n",
        "\r\n",
        "\r\n",
        "def tensor_list_for_board(img_tensors_list):\r\n",
        "    grid_h = len(img_tensors_list)\r\n",
        "    grid_w = max(len(img_tensors) for img_tensors in img_tensors_list)\r\n",
        "\r\n",
        "    batch_size, channel, height, width = tensor_for_board(\r\n",
        "        img_tensors_list[0][0]).size()\r\n",
        "    canvas_h = grid_h * height\r\n",
        "    canvas_w = grid_w * width\r\n",
        "    canvas = torch.FloatTensor(\r\n",
        "        batch_size, channel, canvas_h, canvas_w).fill_(0.5)\r\n",
        "    for i, img_tensors in enumerate(img_tensors_list):\r\n",
        "        for j, img_tensor in enumerate(img_tensors):\r\n",
        "            offset_h = i * height\r\n",
        "            offset_w = j * width\r\n",
        "            tensor = tensor_for_board(img_tensor)\r\n",
        "            canvas[:, :, offset_h: offset_h + height,\r\n",
        "                   offset_w: offset_w + width].copy_(tensor)\r\n",
        "\r\n",
        "    return canvas\r\n",
        "\r\n",
        "\r\n",
        "def board_add_image(board, tag_name, img_tensor, step_count):\r\n",
        "    tensor = tensor_for_board(img_tensor)\r\n",
        "\r\n",
        "    for i, img in enumerate(tensor):\r\n",
        "        board.add_image('%s/%03d' % (tag_name, i), img, step_count)\r\n",
        "\r\n",
        "\r\n",
        "def board_add_images(board, tag_name, img_tensors_list, step_count):\r\n",
        "    tensor = tensor_list_for_board(img_tensors_list)\r\n",
        "\r\n",
        "    for i, img in enumerate(tensor):\r\n",
        "        board.add_image('%s/%03d' % (tag_name, i), img, step_count)\r\n",
        "\r\n",
        "\r\n",
        "def save_images(img_tensors, img_names, save_dir):\r\n",
        "    for img_tensor, img_name in zip(img_tensors, img_names):\r\n",
        "        tensor = (img_tensor.clone()+1)*0.5 * 255\r\n",
        "        tensor = tensor.cpu().clamp(0, 255)\r\n",
        "\r\n",
        "        array = tensor.detach().numpy().astype('uint8')\r\n",
        "        if array.shape[0] == 1:\r\n",
        "            array = array.squeeze(0)\r\n",
        "        elif array.shape[0] == 3:\r\n",
        "            array = array.swapaxes(0, 1).swapaxes(1, 2)\r\n",
        "\r\n",
        "        Image.fromarray(array).save(os.path.join(save_dir, img_name))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9btKwGG2MZ0g"
      },
      "source": [
        "def save_checkpoint(model, save_path):\r\n",
        "    if not os.path.exists(os.path.dirname(save_path)):\r\n",
        "        os.makedirs(os.path.dirname(save_path))\r\n",
        "\r\n",
        "    torch.save(model.cpu().state_dict(), save_path)\r\n",
        "    model.cuda()\r\n",
        "\r\n",
        "\r\n",
        "def load_checkpoint(model, checkpoint_path):\r\n",
        "    if not os.path.exists(checkpoint_path):\r\n",
        "        return\r\n",
        "    model.load_state_dict(torch.load(checkpoint_path))\r\n",
        "    model.cuda()\r\n",
        "\r\n",
        "def dir(path, name):\r\n",
        "    name = osp.join(path, name)\r\n",
        "    if not osp.exists(name):\r\n",
        "        os.makedirs(name)\r\n",
        "    return name"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeNI6py8-ReW"
      },
      "source": [
        "def train_tom(train_loader, model, board, lr = 1e-4, keep_step = 100000, decay_step = 100000, save_count=500, display_count = 100, checkpoint_dir=\"/content/checkpoint\", name = 'TOM'):\r\n",
        "    model.cuda()\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    # criterion\r\n",
        "    criterionL1 = nn.L1Loss()\r\n",
        "    criterionVGG = VGGLoss()\r\n",
        "    criterionMask = nn.L1Loss()\r\n",
        "\r\n",
        "    # optimizer\r\n",
        "    optimizer = torch.optim.Adam(\r\n",
        "        model.parameters(), lr=lr, betas=(0.5, 0.999))\r\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda step: 1.0 -\r\n",
        "                                                  max(0, step - keep_step) / float(decay_step + 1))\r\n",
        "\r\n",
        "    for step in range(keep_step + decay_step):\r\n",
        "        iter_start_time = time.time()\r\n",
        "        inputs = train_loader.next_batch()\r\n",
        "\r\n",
        "        im = inputs['image'].cuda()\r\n",
        "        im_pose = inputs['pose_image']\r\n",
        "        im_h = inputs['head']\r\n",
        "        shape = inputs['shape']\r\n",
        "\r\n",
        "        agnostic = inputs['agnostic'].cuda()\r\n",
        "        c = inputs['cloth'].cuda()\r\n",
        "        cm = inputs['cloth_mask'].cuda()\r\n",
        "        pcm = inputs['parse_cloth_mask'].cuda()\r\n",
        "\r\n",
        "        # outputs = model(torch.cat([agnostic, c], 1))  # CP-VTON\r\n",
        "        outputs = model(torch.cat([agnostic, c, cm], 1))  # CP-VTON+\r\n",
        "        p_rendered, m_composite = torch.split(outputs, 3, 1)\r\n",
        "        p_rendered = torch.tanh(p_rendered)\r\n",
        "        m_composite = torch.sigmoid(m_composite)\r\n",
        "        p_tryon = c * m_composite + p_rendered * (1 - m_composite)\r\n",
        "\r\n",
        "        \"\"\"visuals = [[im_h, shape, im_pose],\r\n",
        "                   [c, cm*2-1, m_composite*2-1],\r\n",
        "                   [p_rendered, p_tryon, im]]\"\"\"  # CP-VTON\r\n",
        "\r\n",
        "        visuals = [[im_h, shape, im_pose],\r\n",
        "                   [c, pcm*2-1, m_composite*2-1],\r\n",
        "                   [p_rendered, p_tryon, im]]  # CP-VTON+\r\n",
        "\r\n",
        "        loss_l1 = criterionL1(p_tryon, im)\r\n",
        "        loss_vgg = criterionVGG(p_tryon, im)\r\n",
        "        # loss_mask = criterionMask(m_composite, cm)  # CP-VTON\r\n",
        "        loss_mask = criterionMask(m_composite, pcm)  # CP-VTON+\r\n",
        "        loss = loss_l1 + loss_vgg + loss_mask\r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        if (step+1) % display_count == 0:\r\n",
        "            board_add_images(board, 'combine', visuals, step+1)\r\n",
        "            board.add_scalar('loss', loss.item(), step+1)\r\n",
        "            board.add_scalar('40*loss_mask', (40*loss_mask).item(), step+1)\r\n",
        "            board.add_scalar('loss_l1', loss_l1.item(), step+1)\r\n",
        "            t = time.time() - iter_start_time\r\n",
        "            print('step: %8d, time: %.3f, loss: %4f, (40*loss_mask): %.8f, loss_l1: %.6f' %\r\n",
        "                  (step+1, t, loss.item(), (40*loss_mask).item(), loss_l1.item()), flush=True)\r\n",
        "        if (step+1) % save_count == 0:\r\n",
        "            save_checkpoint(model, os.path.join(\r\n",
        "                checkpoint_dir, name, 'step_%06d.pth' % (step+1)))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db9LxCgDMpFV"
      },
      "source": [
        "def test_tom(model, test_loader, checkpoint_path = '/content/drive/Shareddrives/Virtuon/Working TOM/checkpoint', name = \"TOM\", model_name = \"PreTrainedTOM\", result_dir = \"results\", mode = 'train'):\r\n",
        "    \r\n",
        "    model_path = osp.join(checkpoint_path, name, model_name + \".pth\")\r\n",
        "    load_checkpoint(model, model_path)\r\n",
        "\r\n",
        "    model.cuda()\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    save_dir = osp.join(result_dir, name, mode)\r\n",
        "\r\n",
        "    if not osp.exists(save_dir):\r\n",
        "        os.makedirs(save_dir)\r\n",
        "\r\n",
        "    try_on_dir = dir(save_dir, 'try-on')\r\n",
        "\r\n",
        "    p_rendered_dir = dir(save_dir, 'p_rendered')\r\n",
        "    \r\n",
        "    m_composite_dir = dir(save_dir, 'm_composite')\r\n",
        "    \r\n",
        "    im_pose_dir = dir(save_dir, 'im_pose')\r\n",
        "    \r\n",
        "    shape_dir = dir(save_dir, 'shape')\r\n",
        "    \r\n",
        "    im_h_dir = dir(save_dir, 'im_h')\r\n",
        "   \r\n",
        "    print('Dataset size: %05d!' % (len(test_loader.dataset)), flush=True)\r\n",
        "    for step, inputs in enumerate(test_loader.data_loader):\r\n",
        "        # iter_start_time = time.time()\r\n",
        "\r\n",
        "        im_names = inputs['im_name']\r\n",
        "        im = inputs['image'].cuda()\r\n",
        "        im_pose = inputs['pose_image']\r\n",
        "        im_h = inputs['head']\r\n",
        "        shape = inputs['shape']\r\n",
        "\r\n",
        "        agnostic = inputs['agnostic'].cuda()\r\n",
        "        c = inputs['cloth'].cuda()\r\n",
        "        cm = inputs['cloth_mask'].cuda()\r\n",
        "\r\n",
        "        # outputs = model(torch.cat([agnostic, c], 1))  # CP-VTON\r\n",
        "        outputs = model(torch.cat([agnostic, c, cm], 1))  # CP-VTON+\r\n",
        "        p_rendered, m_composite = torch.split(outputs, 3, 1)\r\n",
        "        p_rendered = torch.tanh(p_rendered)\r\n",
        "        m_composite = torch.sigmoid(m_composite)\r\n",
        "        p_tryon = c * m_composite + p_rendered * (1 - m_composite)\r\n",
        "\r\n",
        "        # visuals = [[im_h, shape, im_pose],\r\n",
        "        #            [c, 2*cm-1, m_composite],\r\n",
        "        #            [p_rendered, p_tryon, im]]\r\n",
        "\r\n",
        "        save_images(p_tryon, im_names, try_on_dir)\r\n",
        "        save_images(im_h, im_names, im_h_dir)\r\n",
        "        save_images(shape, im_names, shape_dir)\r\n",
        "        save_images(im_pose, im_names, im_pose_dir)\r\n",
        "        save_images(m_composite, im_names, m_composite_dir)\r\n",
        "        save_images(p_rendered, im_names, p_rendered_dir)  # For test data\r\n",
        "\r\n",
        "        # if (step+1) % display_count == 0:\r\n",
        "        # #    board_add_images(board, 'combine', visuals, step+1)\r\n",
        "        #     t = time.time() - iter_start_time\r\n",
        "        #     print('step: %8d, time: %.3f' % (step+1, t), flush=True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_egvk0GKQXw"
      },
      "source": [
        "# from torch.utils.tensorboard.writer import SummaryWriter\r\n",
        "\r\n",
        "# tensorboard_dir = '/content/tensorboard'\r\n",
        "# name = 'TOM'\r\n",
        "model = UnetGenerator( 26, 4, 6, ngf=64, norm_layer=nn.InstanceNorm2d)\r\n",
        "train_dataset = CPDataset(\"TOM\",\"cp-vton-plus\", mode='train')\r\n",
        "train_loader = CPDataLoader(train_dataset, batch=4)\r\n",
        "\r\n",
        "# for viz.\r\n",
        "# if not os.path.exists(tensorboard_dir):\r\n",
        "#         os.makedirs(os.path.join(tensorboard_dir, name))\r\n",
        "# board = SummaryWriter(log_dir=os.path.join(tensorboard_dir, name))\r\n",
        "\r\n",
        "\r\n",
        "# %load_ext tensorboard\r\n",
        "# %tensorboard --logdir /content/tensorboard/TOM\r\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL80c4h0HYjY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee8def53-d631-4cd7-ef5f-437350d4137e"
      },
      "source": [
        "test_tom(model, train_loader, mode='train')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset size: 14221!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pzk-8X2RFJQ"
      },
      "source": [
        "!cp -r /content/checkpoint /content/drive/Shareddrives/Virtuon/Working\\ TOM/\r\n",
        "!cp -r /content/tensorboard /content/drive/Shareddrives/Virtuon/Working\\ TOM/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czX4gkX7Rv9U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}