{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CAGAN With GradientTape.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1DGwQwGLU_-Rng0m5sAHl0KlsOzfQjU0z",
      "authorship_tag": "ABX9TyNHrHcPK9X0OurRhS7QzZaD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arjunparmar/VIRTUON/blob/main/Harshit/CAGAN_With_GradientTape.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaJqjjYIcZB9",
        "outputId": "152af31c-ddaa-4eb3-af9b-2bb35115594d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ci6QFm5XMz4"
      },
      "source": [
        "# !cp /content/drive/Shareddrives/Virtuon/CP_Viton_Plus/viton_plus.zip /content/\n",
        "# !unzip -qq viton_plus.zip"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejfTKzfSjkSC"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2 as cv\n",
        "import PIL\n",
        "from tqdm import tqdm\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow_addons.layers import InstanceNormalization\n",
        "from tensorflow.image import resize\n",
        "from tensorflow import reshape\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img, load_img"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWO6XJS6bn6Z"
      },
      "source": [
        "cloth_train_path = \"/content/train/cloth/\"\n",
        "image_train_path = \"/content/train/image/\"\n",
        "cloth_mask_train_path = \"/content/train/cloth-mask/\"\n",
        "image_mask_train_path = \"/content/train/image-mask/\"\n",
        "pose_train_path = \"/content/train/pose/\""
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giN9K_ZbdHrq"
      },
      "source": [
        "train_data = pd.read_csv(\"train_pairs.txt\", delimiter=\" \", header = None, nrows=100)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iIGHtWSdI_2"
      },
      "source": [
        "def load(path):\n",
        "    temp = img_to_array(load_img(path))\n",
        "    return temp[tf.newaxis,...]/255"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nvG5x0WeXsn"
      },
      "source": [
        "def preprocessing(filename, labelname):\n",
        "    x_i = tf.io.read_file(image_train_path + filename)\n",
        "    x_i = tf.image.decode_jpeg(x_i, channels=3)\n",
        "    x_i = tf.image.convert_image_dtype(x_i, tf.float32)\n",
        "    \n",
        "    y_i = tf.io.read_file(cloth_train_path + labelname)\n",
        "    y_i = tf.image.decode_jpeg(y_i, channels=3)\n",
        "    y_i = tf.image.convert_image_dtype(y_i, tf.float32)\n",
        "\n",
        "    random = np.random.randint(0,len(train_data))\n",
        "    y_j = tf.io.read_file(cloth_train_path + train_data.iloc[random, 1])\n",
        "    y_j = tf.image.decode_jpeg(y_j, channels=3)\n",
        "    y_j = tf.image.convert_image_dtype(y_j, tf.float32)\n",
        "\n",
        "    return tf.concat([x_i, y_i, y_j], axis=-1)\n"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMigQ8lbXJq9"
      },
      "source": [
        "train = tf.data.Dataset.from_tensor_slices((train_data.iloc[:,0], train_data.iloc[:,1]))\n",
        "train = train.map(preprocessing)\n",
        "train = train.shuffle(len(train_data))\n",
        "train = train.batch(8)#.repeat()"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcVMNuN4kgAo"
      },
      "source": [
        "def make_generator(input, n_filters = 64, k_size = 4, stride = 2):\n",
        "    \n",
        "    # Encoder Layer\n",
        "\n",
        "    # Stage 1\n",
        "    e1 = layers.Conv2D(n_filters, kernel_size = k_size, strides = stride, padding = \"same\", name = \"Encoder_1\")(input)\n",
        "    n1 = e1\n",
        "    a1 = layers.LeakyReLU(name = \"Encoder_Activation_1\")(e1)\n",
        "    \n",
        "    # Stage 2\n",
        "    e2 = layers.Conv2D(n_filters*2, kernel_size = k_size, strides = stride, padding = \"same\", name = \"Encoder_2\")(a1)\n",
        "    n2 = InstanceNormalization(name = \"Encoder_Instance_Normalization_2\")(e2)\n",
        "    a2 = layers.LeakyReLU(name = \"Encoder_Activation_2\")(n2)\n",
        "\n",
        "    # Stage 3\n",
        "    e3 = layers.Conv2D(n_filters*4, kernel_size = k_size, strides = stride, padding = \"same\", name = \"Encoder_3\")(a2)\n",
        "    n3 = InstanceNormalization(name = \"Encoder_Instance_Normalization_3\")(e3)\n",
        "    a3 = layers.LeakyReLU(name = \"Encoder_Activation_3\")(n3)\n",
        "\n",
        "    # Stage 4\n",
        "    e4 = layers.Conv2D(n_filters*8, kernel_size = k_size, strides = stride, padding = \"same\", name = \"Encoder_4\")(a3)\n",
        "    # n3 = InstanceNormalization(name = \"Encoder_Instance_Normalization_4\")(e4)\n",
        "    encoder_output = layers.LeakyReLU(name = \"Encoder_Activation_4\")(e4)\n",
        "\n",
        "    # Decoder Layer\n",
        "\n",
        "    # Stage 1\n",
        "    d1 = layers.Conv2DTranspose(n_filters*4, kernel_size = k_size, strides = stride, padding = \"same\", name = \"Decoder_1\")(encoder_output)\n",
        "    d1 = InstanceNormalization(name = \"Decoder_Instance_Normalization_1\")(d1)\n",
        "    con1 = layers.concatenate([d1,n3])\n",
        "    a1 = layers.ReLU(name = \"Decoder_Activation_1\")(con1)\n",
        "\n",
        "    # Stage 2\n",
        "    d2 = layers.Conv2DTranspose(n_filters*2, kernel_size = k_size, strides = stride, padding = \"same\", name = \"Decoder_2\")(a1)\n",
        "    d2 = InstanceNormalization(name = \"Decoder_Instance_Normalization_2\")(d2)\n",
        "    con2 = layers.concatenate([d2,n2])\n",
        "    a2 = layers.ReLU(name = \"Decoder_Activation_2\")(con2)\n",
        "\n",
        "    # Stage 3\n",
        "    d3 = layers.Conv2DTranspose(n_filters, kernel_size = k_size, strides = stride, padding = \"same\", name = \"Decoder_3\")(a2)\n",
        "    d3 = InstanceNormalization(name = \"Decoder_Instance_Normalization_3\")(d3)\n",
        "    con3 = layers.concatenate([d3,n1])\n",
        "    decoder_output = layers.ReLU(name = \"Decoder_Activation_3\")(con3)\n",
        "\n",
        "    # Output Stage\n",
        "    output = layers.Conv2DTranspose(4, kernel_size = 2, strides = stride, padding = \"same\", name = \"Output\", activation='sigmoid')(decoder_output)\n",
        "\n",
        "    return Model(input, output)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3A6zGeQknHG",
        "outputId": "f8066458-c0da-4e26-99cf-d62c95610ba7"
      },
      "source": [
        "generator_input = layers.Input((256,192,9), name = \"Input\")\n",
        "generator = make_generator(generator_input)\n",
        "# tf.keras.utils.plot_model(generator)\n",
        "generator.summary()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input (InputLayer)              [(None, 256, 192, 9) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Encoder_1 (Conv2D)              (None, 128, 96, 64)  9280        Input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder_Activation_1 (LeakyReLU (None, 128, 96, 64)  0           Encoder_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder_2 (Conv2D)              (None, 64, 48, 128)  131200      Encoder_Activation_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Encoder_Instance_Normalization_ (None, 64, 48, 128)  256         Encoder_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder_Activation_2 (LeakyReLU (None, 64, 48, 128)  0           Encoder_Instance_Normalization_2[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder_3 (Conv2D)              (None, 32, 24, 256)  524544      Encoder_Activation_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Encoder_Instance_Normalization_ (None, 32, 24, 256)  512         Encoder_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder_Activation_3 (LeakyReLU (None, 32, 24, 256)  0           Encoder_Instance_Normalization_3[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder_4 (Conv2D)              (None, 16, 12, 512)  2097664     Encoder_Activation_3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Encoder_Activation_4 (LeakyReLU (None, 16, 12, 512)  0           Encoder_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "Decoder_1 (Conv2DTranspose)     (None, 32, 24, 256)  2097408     Encoder_Activation_4[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Decoder_Instance_Normalization_ (None, 32, 24, 256)  512         Decoder_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 32, 24, 512)  0           Decoder_Instance_Normalization_1[\n",
            "                                                                 Encoder_Instance_Normalization_3[\n",
            "__________________________________________________________________________________________________\n",
            "Decoder_Activation_1 (ReLU)     (None, 32, 24, 512)  0           concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Decoder_2 (Conv2DTranspose)     (None, 64, 48, 128)  1048704     Decoder_Activation_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Decoder_Instance_Normalization_ (None, 64, 48, 128)  256         Decoder_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 64, 48, 256)  0           Decoder_Instance_Normalization_2[\n",
            "                                                                 Encoder_Instance_Normalization_2[\n",
            "__________________________________________________________________________________________________\n",
            "Decoder_Activation_2 (ReLU)     (None, 64, 48, 256)  0           concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Decoder_3 (Conv2DTranspose)     (None, 128, 96, 64)  262208      Decoder_Activation_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Decoder_Instance_Normalization_ (None, 128, 96, 64)  128         Decoder_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 128, 96, 128) 0           Decoder_Instance_Normalization_3[\n",
            "                                                                 Encoder_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "Decoder_Activation_3 (ReLU)     (None, 128, 96, 128) 0           concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Output (Conv2DTranspose)        (None, 256, 192, 4)  2052        Decoder_Activation_3[0][0]       \n",
            "==================================================================================================\n",
            "Total params: 6,174,724\n",
            "Trainable params: 6,174,724\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jQCWsg5kqgo"
      },
      "source": [
        "def make_discriminator(input, n_filters = 64, k_size = 4, stride = 2):\n",
        "    \n",
        "    x = layers.Conv2D(n_filters, kernel_size=k_size, strides=stride, name = \"Layer_64\", padding = 'same')(input)\n",
        "    x = layers.LeakyReLU(name = \"Layer_64_Activation\")(x)\n",
        "\n",
        "    x = layers.Conv2D(n_filters*2, kernel_size=k_size, strides=stride, name = \"Layer_128\", padding = 'same')(x)\n",
        "    x = layers.BatchNormalization(name = \"Layer_128_BatchNormalization\")(x)\n",
        "    x = layers.LeakyReLU(name = \"Layer_128_Activation\")(x)\n",
        "\n",
        "    x = layers.Conv2D(n_filters*4, kernel_size=k_size, strides=stride, name = \"Layer_256\", padding = 'same')(x)\n",
        "    x = layers.BatchNormalization(name = \"Layer_256_BatchNormalization\")(x)\n",
        "    x = layers.LeakyReLU(name = \"Layer_256_Activation\")(x)\n",
        "\n",
        "    x = layers.Conv2D(n_filters*8, kernel_size=k_size, strides=1, name = \"Layer_512\", padding = 'same')(x)\n",
        "    x = layers.BatchNormalization(name = \"Layer_512_BatchNormalization\")(x)\n",
        "    x = layers.LeakyReLU(name = \"Layer_512_Activation\")(x)\n",
        "\n",
        "    x = layers.Conv2D(1, kernel_size= 4, strides = 1, name = \"Output\", padding = 'same')(x)\n",
        "\n",
        "    return Model(input, x, name = \"discriminator\")"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIhwlvx2kuX1",
        "outputId": "2ef8912d-46c8-48eb-87e8-c922690222d0"
      },
      "source": [
        "discriminator = make_discriminator(layers.Input((256,192,6), name = \"Input\"))\n",
        "discriminator.summary()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Input (InputLayer)           [(None, 256, 192, 6)]     0         \n",
            "_________________________________________________________________\n",
            "Layer_64 (Conv2D)            (None, 128, 96, 64)       6208      \n",
            "_________________________________________________________________\n",
            "Layer_64_Activation (LeakyRe (None, 128, 96, 64)       0         \n",
            "_________________________________________________________________\n",
            "Layer_128 (Conv2D)           (None, 64, 48, 128)       131200    \n",
            "_________________________________________________________________\n",
            "Layer_128_BatchNormalization (None, 64, 48, 128)       512       \n",
            "_________________________________________________________________\n",
            "Layer_128_Activation (LeakyR (None, 64, 48, 128)       0         \n",
            "_________________________________________________________________\n",
            "Layer_256 (Conv2D)           (None, 32, 24, 256)       524544    \n",
            "_________________________________________________________________\n",
            "Layer_256_BatchNormalization (None, 32, 24, 256)       1024      \n",
            "_________________________________________________________________\n",
            "Layer_256_Activation (LeakyR (None, 32, 24, 256)       0         \n",
            "_________________________________________________________________\n",
            "Layer_512 (Conv2D)           (None, 32, 24, 512)       2097664   \n",
            "_________________________________________________________________\n",
            "Layer_512_BatchNormalization (None, 32, 24, 512)       2048      \n",
            "_________________________________________________________________\n",
            "Layer_512_Activation (LeakyR (None, 32, 24, 512)       0         \n",
            "_________________________________________________________________\n",
            "Output (Conv2D)              (None, 32, 24, 1)         8193      \n",
            "=================================================================\n",
            "Total params: 2,771,393\n",
            "Trainable params: 2,769,601\n",
            "Non-trainable params: 1,792\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bGJDp-Nkxcg"
      },
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KdZ6EuMlCKe"
      },
      "source": [
        "def discriminator_loss_fn(real, fake, fake2):\n",
        "    real_loss = cross_entropy(tf.ones_like(real), real)\n",
        "    generated_loss = cross_entropy(tf.zeros_like(fake), fake)\n",
        "    generated_loss_2 = cross_entropy(tf.zeros_like(fake2), fake2)\n",
        "\n",
        "    total_loss = real_loss + generated_loss + generated_loss_2\n",
        "\n",
        "    return total_loss"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKgImFrMmb1Z"
      },
      "source": [
        "def generator_loss_fn(disc_output):\n",
        "    loss = tf.math.reduce_mean(tf.math.log(disc_output))\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-tgmdg1s2I9"
      },
      "source": [
        "gamma_i = 0.1"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmODb9uvt_RH"
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKDU3qnwnfTY"
      },
      "source": [
        "@tf.function\n",
        "def train_step(gen_input):\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        gen_output = generator(gen_input, training = True)\n",
        "        \n",
        "        x_i = layers.Lambda(lambda x: x[:,:,:, 0:3])(gen_input)\n",
        "        y_i = layers.Lambda(lambda x: x[:,:,:, 3:6])(gen_input)\n",
        "        y_j = layers.Lambda(lambda x: x[:,:,:, 6:])(gen_input)\n",
        "        alpha = layers.Lambda(lambda x:x[:,:,:,:1])(gen_output)\n",
        "        x_i_j = layers.Lambda(lambda x:x[:,:,:,1:])(gen_output)\n",
        "\n",
        "        fake = alpha*x_i_j + (1 - alpha)*x_i\n",
        "\n",
        "        cycle_output = generator(layers.concatenate([fake, y_j, y_i]), training=True)\n",
        "        cycle_alpha = layers.Lambda(lambda x:x[:,:,:,:1])(cycle_output)\n",
        "        cycle_x_i_j = layers.Lambda(lambda x:x[:,:,:,1:])(cycle_output)\n",
        "\n",
        "        cycle_output_x_i = alpha*cycle_x_i_j + (1 - alpha)*fake\n",
        "        \n",
        "        disc_real_image = discriminator(layers.concatenate([x_i, y_i]), training=True)\n",
        "        disc_fake_image = discriminator(layers.concatenate([fake, y_j]), training=True)\n",
        "        disc_fake2_image = discriminator(layers.concatenate([x_i, y_j]), training=True)\n",
        "\n",
        "        discriminator_loss = discriminator_loss_fn(disc_real_image, disc_fake_image, disc_fake2_image)\n",
        "        generator_loss = generator_loss_fn(disc_fake_image)\n",
        "        cycle_loss = tf.math.reduce_mean(tf.math.abs(x_i - cycle_output_x_i))\n",
        "        loss_id = tf.reduce_mean(tf.abs(alpha))\n",
        "\n",
        "        total_generator_loss = generator_loss + 1*(1*cycle_loss + gamma_i*loss_id)\n",
        "        total_discriminator_loss = discriminator_loss*2\n",
        "\n",
        "    generator_gradients = gen_tape.gradient(total_generator_loss, generator.trainable_variables)\n",
        "    discriminator_gradients = disc_tape.gradient(total_discriminator_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))\n",
        "    "
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGlzIb_LDB5m"
      },
      "source": [
        "def generate_images(model = generator):\n",
        "  random = 0\n",
        "  test_input = preprocessing(train_data.iloc[random,0], train_data.iloc[random, 1])\n",
        "  test_input = test_input[tf.newaxis,...]\n",
        "  prediction = model(test_input, training=True)\n",
        "  plt.figure(figsize=(15,15))\n",
        "\n",
        "  display_list = [test_input[0,:,:,:3], test_input[0,:,:,3:6], test_input[0,:,:,6:], prediction[0]]\n",
        "  title = ['Input Image', 'Input Cloth', 'Target Cloth', 'Predicted Image']\n",
        "\n",
        "  for i in range(4):\n",
        "    plt.subplot(1, 4, i+1)\n",
        "    plt.title(title[i])\n",
        "    # getting the pixel values between [0, 1] to plot it.\n",
        "    plt.imshow(display_list[i])\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o360HiEru2D5"
      },
      "source": [
        "def fit(train_ds, epochs):\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        display.clear_output(wait=True)\n",
        "        if epoch%10 == 0:\n",
        "            generate_images()\n",
        "        for n, (input_image) in train_ds.enumerate():\n",
        "            train_step(input_image)"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "1yhVjzOy9OAX",
        "outputId": "d79f16ea-82f4-4ee2-9e24-afb646f03269"
      },
      "source": [
        "fit(train, 500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " 15%|█▌        | 75/500 [02:50<16:06,  2.27s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcGZQScG9Szr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}