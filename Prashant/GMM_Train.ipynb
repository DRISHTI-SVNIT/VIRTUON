{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GMM_Train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arjunparmar/VIRTUON/blob/main/Prashant/GMM_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyFS5LZJJdI_",
        "outputId": "9d97cec9-3b36-49b2-eaa7-4528f098653c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRjZxl_q1QtY"
      },
      "source": [
        "!cp /content/drive/Shareddrives/Virtuon/Pytorch/cp-vton-plus.zip /content/"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ypd6IEiKNybZ"
      },
      "source": [
        "!unzip -qq cp-vton-plus.zip -d /content/"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EriiCYwlH0kT"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "import os\n",
        "import os.path as osp\n",
        "import json"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_iyjB38IcVl"
      },
      "source": [
        "class CPDataset(data.Dataset):\n",
        "    def __init__(self, stage, all_root=\"cp-vton-plus\", data_path = \"data\", mode=\"train\", radius=5, img_height=256, img_width=192):\n",
        "        super(CPDataset, self).__init__()\n",
        "\n",
        "        self.root = all_root\n",
        "\n",
        "        self.data_root = osp.join(all_root,data_path)\n",
        "\n",
        "        self.datamode = mode\n",
        "\n",
        "        self.stage = stage\n",
        "\n",
        "        self.data_list = \"\".join([mode, \"_pairs.txt\"])\n",
        "\n",
        "        self.fine_height = img_height\n",
        "\n",
        "        self.fine_width = img_width\n",
        "\n",
        "        self.radius = radius\n",
        "\n",
        "        self.data_path = osp.join(all_root,data_path, mode)\n",
        "        \n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "        \n",
        "        self.transform_1 = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5), (0.5))\n",
        "        ])\n",
        "\n",
        "        self.transform_2 = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5), (0.5, 0.5))\n",
        "        ])\n",
        "\n",
        "        self.transform_3 = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "        im_names = []\n",
        "        c_names = []\n",
        "\n",
        "        with open(osp.join(self.data_root, self.data_list), 'r') as f:\n",
        "            for line in f.readlines():\n",
        "                im_name, c_name = line.strip().split()\n",
        "                im_names.append(im_name)\n",
        "                c_names.append(c_name)\n",
        "\n",
        "        self.im_names = im_names\n",
        "        self.c_names = c_names\n",
        "\n",
        "    def name(self):\n",
        "        return \"CPDataset\"\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        c_name = self.c_names[index]\n",
        "        im_name = self.im_names[index]\n",
        "        if self.stage == \"GMM\":\n",
        "            c = Image.open(osp.join(self.data_path, 'cloth', c_name))\n",
        "            cm = Image.open(osp.join(self.data_path, 'cloth-mask', c_name)).convert('L')\n",
        "        else:\n",
        "            c = Image.open(osp.join(self.data_path, 'warp-cloth', im_name))\n",
        "            cm = Image.open(osp.join(self.data_path, 'warp-mask', im_name)).convert('L')\n",
        "        \n",
        "        c = self.transform(c)\n",
        "        cm_array = np.array(cm)\n",
        "        cm_array = (cm_array >= 128).astype(np.float32)\n",
        "        cm = torch.from_numpy(cm_array)\n",
        "        cm.unsqueeze_(0)\n",
        "\n",
        "        # person image\n",
        "        im = Image.open(osp.join(self.data_path, 'image', im_name))\n",
        "        im = self.transform(im)\n",
        "\n",
        "        \n",
        "        # LIP labels\n",
        "        \n",
        "        # [(0, 0, 0),    # 0=Background\n",
        "        #  (128, 0, 0),  # 1=Hat\n",
        "        #  (255, 0, 0),  # 2=Hair\n",
        "        #  (0, 85, 0),   # 3=Glove\n",
        "        #  (170, 0, 51),  # 4=SunGlasses\n",
        "        #  (255, 85, 0),  # 5=UpperClothes\n",
        "        #  (0, 0, 85),     # 6=Dress\n",
        "        #  (0, 119, 221),  # 7=Coat\n",
        "        #  (85, 85, 0),    # 8=Socks\n",
        "        #  (0, 85, 85),    # 9=Pants\n",
        "        #  (85, 51, 0),    # 10=Jumpsuits\n",
        "        #  (52, 86, 128),  # 11=Scarf\n",
        "        #  (0, 128, 0),    # 12=Skirt\n",
        "        #  (0, 0, 255),    # 13=Face\n",
        "        #  (51, 170, 221),  # 14=LeftArm\n",
        "        #  (0, 255, 255),   # 15=RightArm\n",
        "        #  (85, 255, 170),  # 16=LeftLeg\n",
        "        #  (170, 255, 85),  # 17=RightLeg\n",
        "        #  (255, 255, 0),   # 18=LeftShoe\n",
        "        #  (255, 170, 0)    # 19=RightShoe\n",
        "        #  (170, 170, 50)   # 20=Skin/Neck/Chest (Newly added after running dataset_neck_skin_correction.py)\n",
        "        #  ]\n",
        "         \n",
        "        # load parsing image\n",
        "        parse_name = im_name.replace('.jpg', '.png')\n",
        "        im_parse = Image.open(osp.join(self.data_path, 'image-parse-new',parse_name)).convert('L')\n",
        "        parse_array = np.array(im_parse)\n",
        "\n",
        "        im_mask = Image.open(osp.join(self.data_path, 'image-mask', parse_name)).convert('L')\n",
        "        mask_array = np.array(im_mask)\n",
        "\n",
        "        parse_shape = (mask_array > 0).astype(np.float32)\n",
        "\n",
        "        if self.stage == 'GMM':\n",
        "            parse_head = (parse_array == 1).astype(np.float32) + (parse_array == 4).astype(np.float32) + (parse_array == 13).astype(np.float32)\n",
        "\n",
        "        else:\n",
        "            parse_head = (parse_array == 1).astype(np.float32) + (parse_array == 2).astype(np.float32) + (parse_array == 4).astype(np.float32) + (parse_array == 9).astype(np.float32) + (parse_array == 12).astype(np.float32) + (parse_array == 13).astype(np.float32) + (parse_array == 16).astype(np.float32) + (parse_array == 17).astype(np.float32)  \n",
        "            \n",
        "        parse_cloth = (parse_array == 5).astype(np.float32) + (parse_array == 6).astype(np.float32) + (parse_array == 7).astype(np.float32)\n",
        "\n",
        "        parse_shape_ori = Image.fromarray((parse_shape*255).astype(np.uint8))\n",
        "\n",
        "        parse_shape = parse_shape_ori.resize((self.fine_width//16, self.fine_height//16), Image.BILINEAR)\n",
        "\n",
        "        parse_shape = parse_shape.resize((self.fine_width, self.fine_height), Image.BILINEAR)\n",
        "        \n",
        "        parse_shape_ori = parse_shape_ori.resize((self.fine_width, self.fine_height), Image.BILINEAR)\n",
        "        \n",
        "        shape_ori = self.transform_1(parse_shape_ori)\n",
        "\n",
        "        shape = self.transform_1(parse_shape)\n",
        "\n",
        "        phead = torch.from_numpy(parse_head)\n",
        "\n",
        "        pcm = torch.from_numpy(parse_cloth)\n",
        "\n",
        "        # Upper Cloth\n",
        "        im_c = im*pcm + (1 - pcm)\n",
        "        im_h = im*phead + (1-phead)\n",
        "\n",
        "        # load pose points\n",
        "        pose_name = im_name.replace('.jpg', '_keypoints.json')\n",
        "        with open(osp.join(self.data_path, 'pose', pose_name), 'r') as f:\n",
        "            pose_label = json.load(f)\n",
        "            pose_data = pose_label['people'][0]['pose_keypoints']\n",
        "            pose_data = np.array(pose_data)\n",
        "            pose_data = pose_data.reshape([-1,3])\n",
        "        \n",
        "        point_num = pose_data.shape[0]\n",
        "        pose_map = torch.zeros(point_num, self.fine_height, self.fine_width)\n",
        "        \n",
        "        r = self.radius\n",
        "        \n",
        "        im_pose = Image.new('L', (self.fine_width, self.fine_height))\n",
        "        pose_draw = ImageDraw.Draw(im_pose)\n",
        "\n",
        "        for i in range(point_num):\n",
        "            one_map = Image.new('L', (self.fine_width, self.fine_height))\n",
        "            draw = ImageDraw.Draw(one_map)\n",
        "            pointx = pose_data[i, 0]\n",
        "            pointy = pose_data[i, 1]\n",
        "\n",
        "            if pointx > 1 and pointy > 1:\n",
        "                draw.rectangle((pointx - r, pointy - r, pointx + r, pointy + r), 'white', 'white')\n",
        "                pose_draw.rectangle((pointx - r, pointy - r, pointx + r, pointy + r), 'white', 'white')\n",
        "\n",
        "            one_map = self.transform_1(one_map)\n",
        "            pose_map[i] = one_map[0]\n",
        "\n",
        "        im_pose = self.transform_1(im_pose)\n",
        "\n",
        "        agnostic = torch.cat([shape, im_h, pose_map], 0)\n",
        "\n",
        "        if self.stage == 'GMM':\n",
        "            im_g = Image.open(osp.join(self.root, 'grid.png'))\n",
        "            im_g = self.transform(im_g)\n",
        "        else:\n",
        "            im_g = ''\n",
        "        \n",
        "        pcm.unsqueeze_(0)\n",
        "        \n",
        "        result = {\n",
        "            'c_name': c_name,\n",
        "            'im_name': im_name,\n",
        "            'cloth': c,\n",
        "            'cloth_mask': cm,\n",
        "            'image': im,\n",
        "            'agnostic': agnostic,\n",
        "            'parse_cloth': im_c,\n",
        "            'shape': shape,\n",
        "            'head': im_h,\n",
        "            'pose_image': im_pose,\n",
        "            'grid_image': im_g,\n",
        "            'parse_cloth_mask': pcm,\n",
        "            'shape_ori': shape_ori,\n",
        "        }\n",
        "\n",
        "        return result\n",
        "    def __len__(self):\n",
        "        return len(self.im_names)\n",
        "\n",
        "\n",
        "class CPDataLoader(object):\n",
        "    def __init__(self, dataset, shuffle=True, batch=4, workers=4):\n",
        "        super(CPDataLoader, self).__init__()\n",
        "\n",
        "        if shuffle:\n",
        "            train_sampler = torch.utils.data.sampler.RandomSampler(dataset)\n",
        "        else:\n",
        "            train_sampler = None\n",
        "        \n",
        "        self.data_loader = torch.utils.data.DataLoader(\n",
        "            dataset, batch_size=batch, shuffle=(train_sampler is None),\n",
        "            num_workers=workers, pin_memory=True, sampler=train_sampler\n",
        "        )\n",
        "        self.dataset = dataset\n",
        "        self.data_iter = self.data_loader.__iter__()\n",
        "\n",
        "    def next_batch(self):\n",
        "        try:\n",
        "            batch = self.data_iter.__next__()\n",
        "        except StopIteration:\n",
        "            self.data_iter = self.data_loader.__iter__()\n",
        "            batch = self.data_iter.__next__()\n",
        "        \n",
        "        return batch\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpl14krN-zCh"
      },
      "source": [
        "class FeatureL2Norm(torch.nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(FeatureL2Norm, self).__init__()\r\n",
        "\r\n",
        "    def forward(self, feature):\r\n",
        "        epsilon = 1e-6\r\n",
        "        norm = torch.pow(torch.sum(torch.pow(feature, 2), 1) +\r\n",
        "                         epsilon, 0.5).unsqueeze(1).expand_as(feature)\r\n",
        "        return torch.div(feature, norm)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuMP6F_rS2o4"
      },
      "source": [
        "class FeatureExtraction(nn.Module):\r\n",
        "    def __init__(self, input_nc, ngf=64, n_layers=3, use_dropout=False):\r\n",
        "        super(FeatureExtraction, self).__init__()\r\n",
        "\r\n",
        "        downconv = nn.Conv2d(input_nc, ngf, kernel_size=4, stride=2, padding=1)\r\n",
        "\r\n",
        "        model = [downconv, nn.ReLU(True), nn.BatchNorm2d(ngf)]\r\n",
        "\r\n",
        "        for i in range(n_layers):\r\n",
        "            in_ngf = 2**i * ngf if 2**i * ngf < 512 else 512\r\n",
        "            out_ngf = 2**(i+1) * ngf if 2**i * ngf < 512 else 512\r\n",
        "            downconv = nn.Conv2d(in_ngf, out_ngf, kernel_size=4, stride=2, padding = 1)\r\n",
        "            model.append(downconv)\r\n",
        "            model.append(nn.ReLU(True))\r\n",
        "            model.append(nn.BatchNorm2d(out_ngf))\r\n",
        "        \r\n",
        "        model.append(nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1))\r\n",
        "        model.append(nn.ReLU(True))\r\n",
        "        model.append(nn.BatchNorm2d(512))\r\n",
        "        model.append(nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1))\r\n",
        "        model.append(nn.ReLU(True))\r\n",
        "\r\n",
        "        self.model = nn.Sequential(*model)\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        return self.model(x)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class FeatureCorrelation(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(FeatureCorrelation, self).__init__()\r\n",
        "    def forward(self, feature_A, feature_B):\r\n",
        "        b,c,h,w = feature_A.size()\r\n",
        "        \r\n",
        "        feature_A = feature_A.transpose(2,3).contiguous().view(b, c, h*w)\r\n",
        "        feature_B = feature_B.contiguous().view(b, c, h*w).transpose(1,2)\r\n",
        "\r\n",
        "        feature_mul = torch.bmm(feature_B, feature_A)\r\n",
        "        correlation_tensor = feature_mul.view(b, h, w, h*w).transpose(2,3).transpose(1,2)\r\n",
        "\r\n",
        "        return correlation_tensor\r\n",
        "        # return feature_mul\r\n",
        "\r\n",
        "class FeatureRegression(nn.Module):\r\n",
        "    def __init__(self, input_nc=512, output_dim=6, use_cuda=True):\r\n",
        "        super(FeatureRegression, self).__init__()\r\n",
        "\r\n",
        "        self.conv = nn.Sequential(\r\n",
        "            nn.Conv2d(input_nc, 512, kernel_size=4, stride=2, padding=1),\r\n",
        "            nn.BatchNorm2d(512),\r\n",
        "            nn.ReLU(True),\r\n",
        "            nn.Conv2d(512, 256, kernel_size=4, stride=2, padding=1),\r\n",
        "            nn.BatchNorm2d(256),\r\n",
        "            nn.ReLU(True),\r\n",
        "            nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\r\n",
        "            nn.BatchNorm2d(128),\r\n",
        "            nn.ReLU(True),\r\n",
        "            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\r\n",
        "            nn.BatchNorm2d(64),\r\n",
        "            nn.ReLU(True)\r\n",
        "        )\r\n",
        "        self.linear = nn.Linear(64 * 4 * 3, output_dim)\r\n",
        "        self.tanh = nn.Tanh()\r\n",
        "        if use_cuda:\r\n",
        "            self.conv.cuda()\r\n",
        "            self.tanh.cuda()\r\n",
        "            self.linear.cuda()\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        x = self.conv(x)\r\n",
        "        # x = x.view(x.size(0), -1)\r\n",
        "        x = x.reshape(x.shape[0], -1)\r\n",
        "        x = self.linear(x)\r\n",
        "        x = self.tanh(x)\r\n",
        "        return x"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6ZPXfNpS36G"
      },
      "source": [
        "class TpsGridGen(nn.Module):\r\n",
        "    def __init__(self, out_h=256, out_w=192, use_regular_grid=True, grid_size=3, reg_factor=0, use_cuda=True):\r\n",
        "        super(TpsGridGen, self).__init__()\r\n",
        "        self.out_h, self.out_w = out_h, out_w\r\n",
        "        self.reg_factor = reg_factor\r\n",
        "        self.use_cuda = use_cuda\r\n",
        "\r\n",
        "        # create grid in numpy\r\n",
        "        self.grid = np.zeros([self.out_h, self.out_w, 3], dtype=np.float32)\r\n",
        "        # sampling grid with dim-0 coords (Y)\r\n",
        "        self.grid_X, self.grid_Y = np.meshgrid(\r\n",
        "            np.linspace(-1, 1, out_w), np.linspace(-1, 1, out_h))\r\n",
        "        # grid_X,grid_Y: size [1,H,W,1,1]\r\n",
        "        self.grid_X = torch.FloatTensor(self.grid_X).unsqueeze(0).unsqueeze(3)\r\n",
        "        self.grid_Y = torch.FloatTensor(self.grid_Y).unsqueeze(0).unsqueeze(3)\r\n",
        "        if use_cuda:\r\n",
        "            self.grid_X = self.grid_X.cuda()\r\n",
        "            self.grid_Y = self.grid_Y.cuda()\r\n",
        "\r\n",
        "        # initialize regular grid for control points P_i\r\n",
        "        if use_regular_grid:\r\n",
        "            axis_coords = np.linspace(-1, 1, grid_size)\r\n",
        "            self.N = grid_size*grid_size\r\n",
        "            P_Y, P_X = np.meshgrid(axis_coords, axis_coords)\r\n",
        "            P_X = np.reshape(P_X, (-1, 1))  # size (N,1)\r\n",
        "            P_Y = np.reshape(P_Y, (-1, 1))  # size (N,1)\r\n",
        "            P_X = torch.FloatTensor(P_X)\r\n",
        "            P_Y = torch.FloatTensor(P_Y)\r\n",
        "            self.P_X_base = P_X.clone()\r\n",
        "            self.P_Y_base = P_Y.clone()\r\n",
        "            self.Li = self.compute_L_inverse(P_X, P_Y).unsqueeze(0)\r\n",
        "            self.P_X = P_X.unsqueeze(2).unsqueeze(\r\n",
        "                3).unsqueeze(4).transpose(0, 4)\r\n",
        "            self.P_Y = P_Y.unsqueeze(2).unsqueeze(\r\n",
        "                3).unsqueeze(4).transpose(0, 4)\r\n",
        "            if use_cuda:\r\n",
        "                self.P_X = self.P_X.cuda()\r\n",
        "                self.P_Y = self.P_Y.cuda()\r\n",
        "                self.P_X_base = self.P_X_base.cuda()\r\n",
        "                self.P_Y_base = self.P_Y_base.cuda()\r\n",
        "\r\n",
        "    def forward(self, theta):\r\n",
        "        warped_grid = self.apply_transformation(\r\n",
        "            theta, torch.cat((self.grid_X, self.grid_Y), 3))\r\n",
        "\r\n",
        "        return warped_grid\r\n",
        "\r\n",
        "    def compute_L_inverse(self, X, Y):\r\n",
        "        N = X.size()[0]  # num of points (along dim 0)\r\n",
        "        # construct matrix K\r\n",
        "        Xmat = X.expand(N, N)\r\n",
        "        Ymat = Y.expand(N, N)\r\n",
        "        P_dist_squared = torch.pow(\r\n",
        "            Xmat-Xmat.transpose(0, 1), 2)+torch.pow(Ymat-Ymat.transpose(0, 1), 2)\r\n",
        "        # make diagonal 1 to avoid NaN in log computation\r\n",
        "        P_dist_squared[P_dist_squared == 0] = 1\r\n",
        "        K = torch.mul(P_dist_squared, torch.log(P_dist_squared))\r\n",
        "        # construct matrix L\r\n",
        "        O = torch.FloatTensor(N, 1).fill_(1)\r\n",
        "        Z = torch.FloatTensor(3, 3).fill_(0)\r\n",
        "        P = torch.cat((O, X, Y), 1)\r\n",
        "        L = torch.cat((torch.cat((K, P), 1), torch.cat(\r\n",
        "            (P.transpose(0, 1), Z), 1)), 0)\r\n",
        "        Li = torch.inverse(L)\r\n",
        "        if self.use_cuda:\r\n",
        "            Li = Li.cuda()\r\n",
        "        return Li\r\n",
        "\r\n",
        "    def apply_transformation(self, theta, points):\r\n",
        "        if theta.dim() == 2:\r\n",
        "            theta = theta.unsqueeze(2).unsqueeze(3)\r\n",
        "        # points should be in the [B,H,W,2] format,\r\n",
        "        # where points[:,:,:,0] are the X coords\r\n",
        "        # and points[:,:,:,1] are the Y coords\r\n",
        "\r\n",
        "        # input are the corresponding control points P_i\r\n",
        "        batch_size = theta.size()[0]\r\n",
        "        # split theta into point coordinates\r\n",
        "        Q_X = theta[:, :self.N, :, :].squeeze(3)\r\n",
        "        Q_Y = theta[:, self.N:, :, :].squeeze(3)\r\n",
        "        Q_X = Q_X + self.P_X_base.expand_as(Q_X)\r\n",
        "        Q_Y = Q_Y + self.P_Y_base.expand_as(Q_Y)\r\n",
        "\r\n",
        "        # get spatial dimensions of points\r\n",
        "        points_b = points.size()[0]\r\n",
        "        points_h = points.size()[1]\r\n",
        "        points_w = points.size()[2]\r\n",
        "\r\n",
        "        # repeat pre-defined control points along spatial dimensions of points to be transformed\r\n",
        "        P_X = self.P_X.expand((1, points_h, points_w, 1, self.N))\r\n",
        "        P_Y = self.P_Y.expand((1, points_h, points_w, 1, self.N))\r\n",
        "\r\n",
        "        # compute weigths for non-linear part\r\n",
        "        W_X = torch.bmm(self.Li[:, :self.N, :self.N].expand(\r\n",
        "            (batch_size, self.N, self.N)), Q_X)\r\n",
        "        W_Y = torch.bmm(self.Li[:, :self.N, :self.N].expand(\r\n",
        "            (batch_size, self.N, self.N)), Q_Y)\r\n",
        "        # reshape\r\n",
        "        # W_X,W,Y: size [B,H,W,1,N]\r\n",
        "        W_X = W_X.unsqueeze(3).unsqueeze(4).transpose(\r\n",
        "            1, 4).repeat(1, points_h, points_w, 1, 1)\r\n",
        "        W_Y = W_Y.unsqueeze(3).unsqueeze(4).transpose(\r\n",
        "            1, 4).repeat(1, points_h, points_w, 1, 1)\r\n",
        "        # compute weights for affine part\r\n",
        "        A_X = torch.bmm(self.Li[:, self.N:, :self.N].expand(\r\n",
        "            (batch_size, 3, self.N)), Q_X)\r\n",
        "        A_Y = torch.bmm(self.Li[:, self.N:, :self.N].expand(\r\n",
        "            (batch_size, 3, self.N)), Q_Y)\r\n",
        "        # reshape\r\n",
        "        # A_X,A,Y: size [B,H,W,1,3]\r\n",
        "        A_X = A_X.unsqueeze(3).unsqueeze(4).transpose(\r\n",
        "            1, 4).repeat(1, points_h, points_w, 1, 1)\r\n",
        "        A_Y = A_Y.unsqueeze(3).unsqueeze(4).transpose(\r\n",
        "            1, 4).repeat(1, points_h, points_w, 1, 1)\r\n",
        "\r\n",
        "        # compute distance P_i - (grid_X,grid_Y)\r\n",
        "        # grid is expanded in point dim 4, but not in batch dim 0, as points P_X,P_Y are fixed for all batch\r\n",
        "        points_X_for_summation = points[:, :, :, 0].unsqueeze(\r\n",
        "            3).unsqueeze(4).expand(points[:, :, :, 0].size()+(1, self.N))\r\n",
        "        points_Y_for_summation = points[:, :, :, 1].unsqueeze(\r\n",
        "            3).unsqueeze(4).expand(points[:, :, :, 1].size()+(1, self.N))\r\n",
        "\r\n",
        "        if points_b == 1:\r\n",
        "            delta_X = points_X_for_summation-P_X\r\n",
        "            delta_Y = points_Y_for_summation-P_Y\r\n",
        "        else:\r\n",
        "            # use expanded P_X,P_Y in batch dimension\r\n",
        "            delta_X = points_X_for_summation - \\\r\n",
        "                P_X.expand_as(points_X_for_summation)\r\n",
        "            delta_Y = points_Y_for_summation - \\\r\n",
        "                P_Y.expand_as(points_Y_for_summation)\r\n",
        "\r\n",
        "        dist_squared = torch.pow(delta_X, 2)+torch.pow(delta_Y, 2)\r\n",
        "        # U: size [1,H,W,1,N]\r\n",
        "        dist_squared[dist_squared == 0] = 1  # avoid NaN in log computation\r\n",
        "        U = torch.mul(dist_squared, torch.log(dist_squared))\r\n",
        "\r\n",
        "        # expand grid in batch dimension if necessary\r\n",
        "        points_X_batch = points[:, :, :, 0].unsqueeze(3)\r\n",
        "        points_Y_batch = points[:, :, :, 1].unsqueeze(3)\r\n",
        "        if points_b == 1:\r\n",
        "            points_X_batch = points_X_batch.expand(\r\n",
        "                (batch_size,)+points_X_batch.size()[1:])\r\n",
        "            points_Y_batch = points_Y_batch.expand(\r\n",
        "                (batch_size,)+points_Y_batch.size()[1:])\r\n",
        "\r\n",
        "        points_X_prime = A_X[:, :, :, :, 0] + \\\r\n",
        "            torch.mul(A_X[:, :, :, :, 1], points_X_batch) + \\\r\n",
        "            torch.mul(A_X[:, :, :, :, 2], points_Y_batch) + \\\r\n",
        "            torch.sum(torch.mul(W_X, U.expand_as(W_X)), 4)\r\n",
        "\r\n",
        "        points_Y_prime = A_Y[:, :, :, :, 0] + \\\r\n",
        "            torch.mul(A_Y[:, :, :, :, 1], points_X_batch) + \\\r\n",
        "            torch.mul(A_Y[:, :, :, :, 2], points_Y_batch) + \\\r\n",
        "            torch.sum(torch.mul(W_Y, U.expand_as(W_Y)), 4)\r\n",
        "\r\n",
        "        return torch.cat((points_X_prime, points_Y_prime), 3)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MJf2qVIVWRs"
      },
      "source": [
        "class DT(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(DT, self).__init__()\r\n",
        "\r\n",
        "    def forward(self, x1, x2):\r\n",
        "        dt = torch.abs(x1 - x2)\r\n",
        "        return dt\r\n",
        "\r\n",
        "\r\n",
        "class DT2(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(DT, self).__init__()\r\n",
        "\r\n",
        "    def forward(self, x1, y1, x2, y2):\r\n",
        "        dt = torch.sqrt(torch.mul(x1 - x2, x1 - x2) +\r\n",
        "                        torch.mul(y1 - y2, y1 - y2))\r\n",
        "        return dt\r\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37fnGH4CahWO"
      },
      "source": [
        "class AffineGridGen(nn.Module):\r\n",
        "    def __init__(self, out_h=256, out_w=192, out_ch=3):\r\n",
        "        super(AffineGridGen, self).__init__()\r\n",
        "        self.out_h = out_h\r\n",
        "        self.out_w = out_w\r\n",
        "        self.out_ch = out_ch\r\n",
        "\r\n",
        "    def forward(self, theta):\r\n",
        "        theta = theta.contiguous()\r\n",
        "        batch_size = theta.size()[0]\r\n",
        "        out_size = torch.Size(\r\n",
        "            (batch_size, self.out_ch, self.out_h, self.out_w))\r\n",
        "        return F.affine_grid(theta, out_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7S40Ub-T2wv"
      },
      "source": [
        "class GicLoss(nn.Module):\r\n",
        "    def __init__(self, fine_height=256, fine_width=192):\r\n",
        "        super(GicLoss, self).__init__()\r\n",
        "        self.dT = DT()\r\n",
        "        self.fine_height = fine_height\r\n",
        "        self.fine_width = fine_width\r\n",
        "\r\n",
        "    def forward(self, grid):\r\n",
        "        Gx = grid[:, :, :, 0]\r\n",
        "        Gy = grid[:, :, :, 1]\r\n",
        "        Gxcenter = Gx[:, 1:self.fine_height - 1, 1:self.fine_width - 1]\r\n",
        "        Gxup = Gx[:, 0:self.fine_height - 2, 1:self.fine_width - 1]\r\n",
        "        Gxdown = Gx[:, 2:self.fine_height, 1:self.fine_width - 1]\r\n",
        "        Gxleft = Gx[:, 1:self.fine_height - 1, 0:self.fine_width - 2]\r\n",
        "        Gxright = Gx[:, 1:self.fine_height - 1, 2:self.fine_width]\r\n",
        "\r\n",
        "        Gycenter = Gy[:, 1:self.fine_height - 1, 1:self.fine_width - 1]\r\n",
        "        Gyup = Gy[:, 0:self.fine_height - 2, 1:self.fine_width - 1]\r\n",
        "        Gydown = Gy[:, 2:self.fine_height, 1:self.fine_width - 1]\r\n",
        "        Gyleft = Gy[:, 1:self.fine_height - 1, 0:self.fine_width - 2]\r\n",
        "        Gyright = Gy[:, 1:self.fine_height - 1, 2:self.fine_width]\r\n",
        "\r\n",
        "        dtleft = self.dT(Gxleft, Gxcenter)\r\n",
        "        dtright = self.dT(Gxright, Gxcenter)\r\n",
        "        dtup = self.dT(Gyup, Gycenter)\r\n",
        "        dtdown = self.dT(Gydown, Gycenter)\r\n",
        "\r\n",
        "        return torch.sum(torch.abs(dtleft - dtright) + torch.abs(dtup - dtdown))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk7M4A93TFxp"
      },
      "source": [
        "class GMM(nn.Module):\r\n",
        "    def __init__(self, grid_size = 5, fine_height=256, fine_width=192):\r\n",
        "        super(GMM, self).__init__()\r\n",
        "        self.extractionA = FeatureExtraction(22, ngf=64, n_layers=3)\r\n",
        "        self.extractionB = FeatureExtraction(1, ngf=64, n_layers=3)\r\n",
        "        self.l2norm = FeatureL2Norm()\r\n",
        "        self.correlation = FeatureCorrelation()\r\n",
        "        self.regression = FeatureRegression(input_nc=192, output_dim=2*grid_size**2, use_cuda=True)\r\n",
        "        self.gridGen = TpsGridGen(fine_height, fine_width, use_cuda=True, grid_size=grid_size)\r\n",
        "\r\n",
        "    def forward(self, inputA, inputB):\r\n",
        "        featureA = self.extractionA(inputA)\r\n",
        "        featureB = self.extractionB(inputB)\r\n",
        "        featureA = self.l2norm(featureA)\r\n",
        "        featureB = self.l2norm(featureB)\r\n",
        "        correlation = self.correlation(featureA.cuda(), featureB.cuda())\r\n",
        "\r\n",
        "        theta = self.regression(correlation)\r\n",
        "        grid = self.gridGen(theta)\r\n",
        "        return grid, theta"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GlB7I5KTc3v"
      },
      "source": [
        "\r\n",
        "def train_gmm(train_loader, model):\r\n",
        "    model.cuda()\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    # criterion\r\n",
        "    criterionL1 = nn.L1Loss()\r\n",
        "    gicloss = GicLoss()\r\n",
        "    # optimizer\r\n",
        "    optimizer = torch.optim.Adam(\r\n",
        "        model.parameters(), lr=0.0001, betas=(0.5, 0.999))\r\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda step: 1.0 -\r\n",
        "                                                  max(0, step - 100000) / float(100000 + 1))\r\n",
        "\r\n",
        "    for step in range(200000):\r\n",
        "        iter_start_time = time.time()\r\n",
        "        inputs = train_loader.next_batch()\r\n",
        "\r\n",
        "        im = inputs['image'].cuda()\r\n",
        "        im_pose = inputs['pose_image'].cuda()\r\n",
        "        im_h = inputs['head'].cuda()\r\n",
        "        shape = inputs['shape'].cuda()\r\n",
        "        agnostic = inputs['agnostic'].cuda()\r\n",
        "        c = inputs['cloth'].cuda()\r\n",
        "        cm = inputs['cloth_mask'].cuda()\r\n",
        "        im_c = inputs['parse_cloth'].cuda()\r\n",
        "        im_g = inputs['grid_image'].cuda()\r\n",
        "\r\n",
        "        grid, theta = model(agnostic, cm)    # can be added c too for new training\r\n",
        "        warped_cloth = F.grid_sample(c, grid, padding_mode='border')\r\n",
        "        warped_mask = F.grid_sample(cm, grid, padding_mode='zeros')\r\n",
        "        warped_grid = F.grid_sample(im_g, grid, padding_mode='zeros')\r\n",
        "\r\n",
        "        visuals = [[im_h, shape, im_pose],\r\n",
        "                   [c, warped_cloth, im_c],\r\n",
        "                   [warped_grid, (warped_cloth+im)*0.5, im]]\r\n",
        "\r\n",
        "        # Lwarp = criterionL1(warped_cloth, im_c)    # loss for warped cloth\r\n",
        "        Lwarp = criterionL1(warped_mask, cm)    # loss for warped mask thank xuxiaochun025 for fixing the git code.\r\n",
        "        # grid regularization loss\r\n",
        "        Lgic = gicloss(grid)\r\n",
        "        # 200x200 = 40.000 * 0.001\r\n",
        "        Lgic = Lgic / (grid.shape[0] * grid.shape[1] * grid.shape[2])\r\n",
        "\r\n",
        "        loss = Lwarp + 40 * Lgic    # total GMM loss\r\n",
        "\r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        # if (step+1) % opt.display_count == 0:\r\n",
        "        #     board_add_images(board, 'combine', visuals, step+1)\r\n",
        "        #     board.add_scalar('loss', loss.item(), step+1)\r\n",
        "        #     board.add_scalar('40*Lgic', (40*Lgic).item(), step+1)\r\n",
        "        #     board.add_scalar('Lwarp', Lwarp.item(), step+1)\r\n",
        "        #     t = time.time() - iter_start_time\r\n",
        "        #     print('step: %8d, time: %.3f, loss: %4f, (40*Lgic): %.8f, Lwarp: %.6f' %\r\n",
        "        #           (step+1, t, loss.item(), (40*Lgic).item(), Lwarp.item()), flush=True)\r\n",
        "\r\n",
        "        # if (step+1) % opt.save_count == 0:\r\n",
        "        #     save_checkpoint(model, os.path.join(\r\n",
        "        #         opt.checkpoint_dir, opt.name, 'step_%06d.pth' % (step+1)))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL80c4h0HYjY"
      },
      "source": [
        "def main():\r\n",
        "  \r\n",
        "    # create dataset\r\n",
        "    train_dataset = CPDataset(\"GMM\",\"cp-vton-plus\")\r\n",
        "\r\n",
        "    # create dataloader\r\n",
        "    train_loader = CPDataLoader(train_dataset)\r\n",
        "\r\n",
        "    model = GMM()\r\n",
        "       \r\n",
        "    train_gmm(train_loader, model)\r\n",
        "       \r\n",
        "\r\n",
        "\r\n",
        "if __name__ == \"__main__\":\r\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pzk-8X2RFJQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czX4gkX7Rv9U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}