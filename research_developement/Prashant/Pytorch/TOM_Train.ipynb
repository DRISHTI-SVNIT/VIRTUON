{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TOM_DataPreprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arjunparmar/VIRTUON/blob/main/Prashant/Pytorch/TOM_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyFS5LZJJdI_",
        "outputId": "403c0c84-29b2-463e-dd80-1145af9812e5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRjZxl_q1QtY"
      },
      "source": [
        "!cp /content/drive/Shareddrives/Virtuon/Pytorch/cp-vton-plus.zip /content/"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ypd6IEiKNybZ"
      },
      "source": [
        "!unzip -qq cp-vton-plus.zip -d /content/"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EriiCYwlH0kT"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import os\n",
        "import os.path as osp\n",
        "import json"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_iyjB38IcVl"
      },
      "source": [
        "class CPDataset(data.Dataset):\n",
        "    def __init__(self, stage, all_root=\"cp-vton-plus\", data_path = \"data\", mode=\"train\", radius=5, img_height=256, img_width=192):\n",
        "        super(CPDataset, self).__init__()\n",
        "\n",
        "        self.root = all_root\n",
        "\n",
        "        self.data_root = osp.join(all_root,data_path)\n",
        "\n",
        "        self.datamode = mode\n",
        "\n",
        "        self.stage = stage\n",
        "\n",
        "        self.data_list = \"\".join([mode, \"_pairs.txt\"])\n",
        "\n",
        "        self.fine_height = img_height\n",
        "\n",
        "        self.fine_width = img_width\n",
        "\n",
        "        self.radius = radius\n",
        "\n",
        "        self.data_path = osp.join(all_root,data_path, mode)\n",
        "        \n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "        \n",
        "        self.transform_1 = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5), (0.5))\n",
        "        ])\n",
        "\n",
        "        self.transform_2 = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5), (0.5, 0.5))\n",
        "        ])\n",
        "\n",
        "        self.transform_3 = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "        im_names = []\n",
        "        c_names = []\n",
        "\n",
        "        with open(osp.join(self.data_root, self.data_list), 'r') as f:\n",
        "            for line in f.readlines():\n",
        "                im_name, c_name = line.strip().split()\n",
        "                im_names.append(im_name)\n",
        "                c_names.append(c_name)\n",
        "\n",
        "        self.im_names = im_names\n",
        "        self.c_names = c_names\n",
        "\n",
        "    def name(self):\n",
        "        return \"CPDataset\"\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        c_name = self.c_names[index]\n",
        "        im_name = self.im_names[index]\n",
        "        if self.stage == \"GMM\":\n",
        "            c = Image.open(osp.join(self.data_path, 'cloth', c_name))\n",
        "            cm = Image.open(osp.join(self.data_path, 'cloth-mask', c_name)).convert('L')\n",
        "        else:\n",
        "            c = Image.open(osp.join(self.data_path, 'warp-cloth', im_name))\n",
        "            cm = Image.open(osp.join(self.data_path, 'warp-mask', im_name)).convert('L')\n",
        "        \n",
        "        c = self.transform(c)\n",
        "        cm_array = np.array(cm)\n",
        "        cm_array = (cm_array >= 128).astype(np.float32)\n",
        "        cm = torch.from_numpy(cm_array)\n",
        "        cm.unsqueeze_(0)\n",
        "\n",
        "        # person image\n",
        "        im = Image.open(osp.join(self.data_path, 'image', im_name))\n",
        "        im = self.transform(im)\n",
        "\n",
        "        \n",
        "        # LIP labels\n",
        "        \n",
        "        # [(0, 0, 0),    # 0=Background\n",
        "        #  (128, 0, 0),  # 1=Hat\n",
        "        #  (255, 0, 0),  # 2=Hair\n",
        "        #  (0, 85, 0),   # 3=Glove\n",
        "        #  (170, 0, 51),  # 4=SunGlasses\n",
        "        #  (255, 85, 0),  # 5=UpperClothes\n",
        "        #  (0, 0, 85),     # 6=Dress\n",
        "        #  (0, 119, 221),  # 7=Coat\n",
        "        #  (85, 85, 0),    # 8=Socks\n",
        "        #  (0, 85, 85),    # 9=Pants\n",
        "        #  (85, 51, 0),    # 10=Jumpsuits\n",
        "        #  (52, 86, 128),  # 11=Scarf\n",
        "        #  (0, 128, 0),    # 12=Skirt\n",
        "        #  (0, 0, 255),    # 13=Face\n",
        "        #  (51, 170, 221),  # 14=LeftArm\n",
        "        #  (0, 255, 255),   # 15=RightArm\n",
        "        #  (85, 255, 170),  # 16=LeftLeg\n",
        "        #  (170, 255, 85),  # 17=RightLeg\n",
        "        #  (255, 255, 0),   # 18=LeftShoe\n",
        "        #  (255, 170, 0)    # 19=RightShoe\n",
        "        #  (170, 170, 50)   # 20=Skin/Neck/Chest (Newly added after running dataset_neck_skin_correction.py)\n",
        "        #  ]\n",
        "         \n",
        "        # load parsing image\n",
        "        parse_name = im_name.replace('.jpg', '.png')\n",
        "        im_parse = Image.open(osp.join(self.data_path, 'image-parse-new',parse_name)).convert('L')\n",
        "        parse_array = np.array(im_parse)\n",
        "\n",
        "        im_mask = Image.open(osp.join(self.data_path, 'image-mask', parse_name)).convert('L')\n",
        "        mask_array = np.array(im_mask)\n",
        "\n",
        "        parse_shape = (mask_array > 0).astype(np.float32)\n",
        "\n",
        "        if self.stage == 'GMM':\n",
        "            parse_head = (parse_array == 1).astype(np.float32) + (parse_array == 4).astype(np.float32) + (parse_array == 13).astype(np.float32)\n",
        "\n",
        "        else:\n",
        "            parse_head = (parse_array == 1).astype(np.float32) + (parse_array == 2).astype(np.float32) + (parse_array == 4).astype(np.float32) + (parse_array == 9).astype(np.float32) + (parse_array == 12).astype(np.float32) + (parse_array == 13).astype(np.float32) + (parse_array == 16).astype(np.float32) + (parse_array == 17).astype(np.float32)  \n",
        "            \n",
        "        parse_cloth = (parse_array == 5).astype(np.float32) + (parse_array == 6).astype(np.float32) + (parse_array == 7).astype(np.float32)\n",
        "\n",
        "        parse_shape_ori = Image.fromarray((parse_shape*255).astype(np.uint8))\n",
        "\n",
        "        parse_shape = parse_shape_ori.resize((self.fine_width//16, self.fine_height//16), Image.BILINEAR)\n",
        "\n",
        "        parse_shape = parse_shape.resize((self.fine_width, self.fine_height), Image.BILINEAR)\n",
        "        \n",
        "        parse_shape_ori = parse_shape_ori.resize((self.fine_width, self.fine_height), Image.BILINEAR)\n",
        "        \n",
        "        shape_ori = self.transform_1(parse_shape_ori)\n",
        "\n",
        "        shape = self.transform_1(parse_shape)\n",
        "\n",
        "        phead = torch.from_numpy(parse_head)\n",
        "\n",
        "        pcm = torch.from_numpy(parse_cloth)\n",
        "\n",
        "        # Upper Cloth\n",
        "        im_c = im*pcm + (1 - pcm)\n",
        "        im_h = im*phead + (1-phead)\n",
        "\n",
        "        # load pose points\n",
        "        pose_name = im_name.replace('.jpg', '_keypoints.json')\n",
        "        with open(osp.join(self.data_path, 'pose', pose_name), 'r') as f:\n",
        "            pose_label = json.load(f)\n",
        "            pose_data = pose_label['people'][0]['pose_keypoints']\n",
        "            pose_data = np.array(pose_data)\n",
        "            pose_data = pose_data.reshape([-1,3])\n",
        "        \n",
        "        point_num = pose_data.shape[0]\n",
        "        pose_map = torch.zeros(point_num, self.fine_height, self.fine_width)\n",
        "        \n",
        "        r = self.radius\n",
        "        \n",
        "        im_pose = Image.new('L', (self.fine_width, self.fine_height))\n",
        "        pose_draw = ImageDraw.Draw(im_pose)\n",
        "\n",
        "        for i in range(point_num):\n",
        "            one_map = Image.new('L', (self.fine_width, self.fine_height))\n",
        "            draw = ImageDraw.Draw(one_map)\n",
        "            pointx = pose_data[i, 0]\n",
        "            pointy = pose_data[i, 1]\n",
        "\n",
        "            if pointx > 1 and pointy > 1:\n",
        "                draw.rectangle((pointx - r, pointy - r, pointx + r, pointy + r), 'white', 'white')\n",
        "                pose_draw.rectangle((pointx - r, pointy - r, pointx + r, pointy + r), 'white', 'white')\n",
        "\n",
        "            one_map = self.transform_1(one_map)\n",
        "            pose_map[i] = one_map[0]\n",
        "\n",
        "        im_pose = self.transform_1(im_pose)\n",
        "\n",
        "        agnostic = torch.cat([shape, im_h, pose_map], 0)\n",
        "\n",
        "        if self.stage == 'GMM':\n",
        "            im_g = Image.open(osp.join(self.root, 'grid.png'))\n",
        "            im_g = self.transform(im_g)\n",
        "        else:\n",
        "            im_g = ''\n",
        "        \n",
        "        pcm.unsqueeze_(0)\n",
        "        \n",
        "        result = {\n",
        "            'c_name': c_name,\n",
        "            'im_name': im_name,\n",
        "            'cloth': c,\n",
        "            'cloth_mask': cm,\n",
        "            'image': im,\n",
        "            'agnostic': agnostic,\n",
        "            'parse_cloth': im_c,\n",
        "            'shape': shape,\n",
        "            'head': im_h,\n",
        "            'pose_image': im_pose,\n",
        "            'grid_image': im_g,\n",
        "            'parse_cloth_mask': pcm,\n",
        "            'shape_ori': shape_ori,\n",
        "        }\n",
        "\n",
        "        return result\n",
        "    def __len__(self):\n",
        "        return len(self.im_names)\n",
        "\n",
        "\n",
        "class CPDataLoader(object):\n",
        "    def __init__(self, dataset, shuffle=True, batch=4, workers=4):\n",
        "        super(CPDataLoader, self).__init__()\n",
        "\n",
        "        if shuffle:\n",
        "            train_sampler = torch.utils.data.sampler.RandomSampler(dataset)\n",
        "        else:\n",
        "            train_sampler = None\n",
        "        \n",
        "        self.data_loader = torch.utils.data.DataLoader(\n",
        "            dataset, batch_size=batch, shuffle=(train_sampler is None),\n",
        "            num_workers=workers, pin_memory=True, sampler=train_sampler\n",
        "        )\n",
        "        self.dataset = dataset\n",
        "        self.data_iter = self.data_loader.__iter__()\n",
        "\n",
        "    def next_batch(self):\n",
        "        try:\n",
        "            batch = self.data_iter.__next__()\n",
        "        except StopIteration:\n",
        "            self.data_iter = self.data_loader.__iter__()\n",
        "            batch = self.data_iter.__next__()\n",
        "        \n",
        "        return batch\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpl14krN-zCh"
      },
      "source": [
        "class Vgg19(nn.Module):\r\n",
        "    def __init__(self, requires_grad=False):\r\n",
        "        super(Vgg19, self).__init__()\r\n",
        "        vgg_pretrained_features = models.vgg19(pretrained=True).features\r\n",
        "        self.slice1 = torch.nn.Sequential()\r\n",
        "        self.slice2 = torch.nn.Sequential()\r\n",
        "        self.slice3 = torch.nn.Sequential()\r\n",
        "        self.slice4 = torch.nn.Sequential()\r\n",
        "        self.slice5 = torch.nn.Sequential()\r\n",
        "        for x in range(2):\r\n",
        "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\r\n",
        "        for x in range(2, 7):\r\n",
        "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\r\n",
        "        for x in range(7, 12):\r\n",
        "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\r\n",
        "        for x in range(12, 21):\r\n",
        "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\r\n",
        "        for x in range(21, 30):\r\n",
        "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\r\n",
        "        if not requires_grad:\r\n",
        "            for param in self.parameters():\r\n",
        "                param.requires_grad = False\r\n",
        "\r\n",
        "    def forward(self, X):\r\n",
        "        h_relu1 = self.slice1(X)\r\n",
        "        h_relu2 = self.slice2(h_relu1)\r\n",
        "        h_relu3 = self.slice3(h_relu2)\r\n",
        "        h_relu4 = self.slice4(h_relu3)\r\n",
        "        h_relu5 = self.slice5(h_relu4)\r\n",
        "        out = [h_relu1, h_relu2, h_relu3, h_relu4, h_relu5]\r\n",
        "        return out\r\n",
        "\r\n",
        "\r\n",
        "class VGGLoss(nn.Module):\r\n",
        "    def __init__(self, layids=None):\r\n",
        "        super(VGGLoss, self).__init__()\r\n",
        "        self.vgg = Vgg19()\r\n",
        "        self.vgg.cuda()\r\n",
        "        self.criterion = nn.L1Loss()\r\n",
        "        self.weights = [1.0/32, 1.0/16, 1.0/8, 1.0/4, 1.0]\r\n",
        "        self.layids = layids\r\n",
        "\r\n",
        "    def forward(self, x, y):\r\n",
        "        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\r\n",
        "        loss = 0\r\n",
        "        if self.layids is None:\r\n",
        "            self.layids = list(range(len(x_vgg)))\r\n",
        "        for i in self.layids:\r\n",
        "            loss += self.weights[i] * \\\r\n",
        "                self.criterion(x_vgg[i], y_vgg[i].detach())\r\n",
        "        return loss\r\n",
        "\r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkZ2YFwkD927"
      },
      "source": [
        "class UnetGenerator(nn.Module):\r\n",
        "    def __init__(self, input_nc, output_nc, num_downs, ngf=64,\r\n",
        "                 norm_layer=nn.BatchNorm2d, use_dropout=False):\r\n",
        "        super(UnetGenerator, self).__init__()\r\n",
        "        # construct unet structure\r\n",
        "        unet_block = UnetSkipConnectionBlock(\r\n",
        "            ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True)\r\n",
        "        for i in range(num_downs - 5):\r\n",
        "            unet_block = UnetSkipConnectionBlock(\r\n",
        "                ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\r\n",
        "        unet_block = UnetSkipConnectionBlock(\r\n",
        "            ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\r\n",
        "        unet_block = UnetSkipConnectionBlock(\r\n",
        "            ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\r\n",
        "        unet_block = UnetSkipConnectionBlock(\r\n",
        "            ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\r\n",
        "        unet_block = UnetSkipConnectionBlock(\r\n",
        "            output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)\r\n",
        "\r\n",
        "        self.model = unet_block\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        return self.model(input)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPmnaCno-84_"
      },
      "source": [
        "class UnetSkipConnectionBlock(nn.Module):\r\n",
        "    def __init__(self, outer_nc, inner_nc, input_nc=None,\r\n",
        "                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\r\n",
        "        super(UnetSkipConnectionBlock, self).__init__()\r\n",
        "        self.outermost = outermost\r\n",
        "        use_bias = norm_layer == nn.InstanceNorm2d\r\n",
        "\r\n",
        "        if input_nc is None:\r\n",
        "            input_nc = outer_nc\r\n",
        "        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\r\n",
        "                             stride=2, padding=1, bias=use_bias)\r\n",
        "        downrelu = nn.LeakyReLU(0.2, True)\r\n",
        "        downnorm = norm_layer(inner_nc)\r\n",
        "        uprelu = nn.ReLU(True)\r\n",
        "        upnorm = norm_layer(outer_nc)\r\n",
        "\r\n",
        "        if outermost:\r\n",
        "            upsample = nn.Upsample(scale_factor=2, mode='bilinear')\r\n",
        "            upconv = nn.Conv2d(inner_nc * 2, outer_nc,\r\n",
        "                               kernel_size=3, stride=1, padding=1, bias=use_bias)\r\n",
        "            down = [downconv]\r\n",
        "            up = [uprelu, upsample, upconv, upnorm]\r\n",
        "            model = down + [submodule] + up\r\n",
        "        elif innermost:\r\n",
        "            upsample = nn.Upsample(scale_factor=2, mode='bilinear')\r\n",
        "            upconv = nn.Conv2d(inner_nc, outer_nc, kernel_size=3,\r\n",
        "                               stride=1, padding=1, bias=use_bias)\r\n",
        "            down = [downrelu, downconv]\r\n",
        "            up = [uprelu, upsample, upconv, upnorm]\r\n",
        "            model = down + up\r\n",
        "        else:\r\n",
        "            upsample = nn.Upsample(scale_factor=2, mode='bilinear')\r\n",
        "            upconv = nn.Conv2d(inner_nc*2, outer_nc, kernel_size=3,\r\n",
        "                               stride=1, padding=1, bias=use_bias)\r\n",
        "            down = [downrelu, downconv, downnorm]\r\n",
        "            up = [uprelu, upsample, upconv, upnorm]\r\n",
        "\r\n",
        "            if use_dropout:\r\n",
        "                model = down + [submodule] + up + [nn.Dropout(0.5)]\r\n",
        "            else:\r\n",
        "                model = down + [submodule] + up\r\n",
        "\r\n",
        "        self.model = nn.Sequential(*model)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        if self.outermost:\r\n",
        "            return self.model(x)\r\n",
        "        else:\r\n",
        "            return torch.cat([x, self.model(x)], 1)\r\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeNI6py8-ReW"
      },
      "source": [
        "def train_tom(train_loader, model):\r\n",
        "    model.cuda()\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    # criterion\r\n",
        "    criterionL1 = nn.L1Loss()\r\n",
        "    criterionVGG = VGGLoss()\r\n",
        "    criterionMask = nn.L1Loss()\r\n",
        "\r\n",
        "    # optimizer\r\n",
        "    optimizer = torch.optim.Adam(\r\n",
        "        model.parameters(), lr=0.0001, betas=(0.5, 0.999))\r\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda step: 1.0 -\r\n",
        "                                                  max(0, step - 100000) / float(100000 + 1))\r\n",
        "\r\n",
        "    for step in range(200000):\r\n",
        "        iter_start_time = time.time()\r\n",
        "        inputs = train_loader.next_batch()\r\n",
        "\r\n",
        "        im = inputs['image'].cuda()\r\n",
        "        im_pose = inputs['pose_image']\r\n",
        "        im_h = inputs['head']\r\n",
        "        shape = inputs['shape']\r\n",
        "\r\n",
        "        agnostic = inputs['agnostic'].cuda()\r\n",
        "        c = inputs['cloth'].cuda()\r\n",
        "        cm = inputs['cloth_mask'].cuda()\r\n",
        "        pcm = inputs['parse_cloth_mask'].cuda()\r\n",
        "\r\n",
        "        # outputs = model(torch.cat([agnostic, c], 1))  # CP-VTON\r\n",
        "        outputs = model(torch.cat([agnostic, c, cm], 1))  # CP-VTON+\r\n",
        "        p_rendered, m_composite = torch.split(outputs, 3, 1)\r\n",
        "        p_rendered = F.tanh(p_rendered)\r\n",
        "        m_composite = F.sigmoid(m_composite)\r\n",
        "        p_tryon = c * m_composite + p_rendered * (1 - m_composite)\r\n",
        "\r\n",
        "        \"\"\"visuals = [[im_h, shape, im_pose],\r\n",
        "                   [c, cm*2-1, m_composite*2-1],\r\n",
        "                   [p_rendered, p_tryon, im]]\"\"\"  # CP-VTON\r\n",
        "\r\n",
        "        visuals = [[im_h, shape, im_pose],\r\n",
        "                   [c, pcm*2-1, m_composite*2-1],\r\n",
        "                   [p_rendered, p_tryon, im]]  # CP-VTON+\r\n",
        "\r\n",
        "        loss_l1 = criterionL1(p_tryon, im)\r\n",
        "        loss_vgg = criterionVGG(p_tryon, im)\r\n",
        "        # loss_mask = criterionMask(m_composite, cm)  # CP-VTON\r\n",
        "        loss_mask = criterionMask(m_composite, pcm)  # CP-VTON+\r\n",
        "        loss = loss_l1 + loss_vgg + loss_mask\r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        # if (step+1) % opt.display_count == 0:\r\n",
        "        #     board_add_images(board, 'combine', visuals, step+1)\r\n",
        "        #     board.add_scalar('metric', loss.item(), step+1)\r\n",
        "        #     board.add_scalar('L1', loss_l1.item(), step+1)\r\n",
        "        #     board.add_scalar('VGG', loss_vgg.item(), step+1)\r\n",
        "        #     board.add_scalar('MaskL1', loss_mask.item(), step+1)\r\n",
        "        #     t = time.time() - iter_start_time\r\n",
        "        #     print('step: %8d, time: %.3f, loss: %.4f, l1: %.4f, vgg: %.4f, mask: %.4f'\r\n",
        "        #           % (step+1, t, loss.item(), loss_l1.item(),\r\n",
        "        #              loss_vgg.item(), loss_mask.item()), flush=True)\r\n",
        "\r\n",
        "        # if (step+1) % opt.save_count == 0:\r\n",
        "        #     save_checkpoint(model, os.path.join(\r\n",
        "        #         opt.checkpoint_dir, opt.name, 'step_%06d.pth' % (step+1)))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cL80c4h0HYjY"
      },
      "source": [
        "def main():\r\n",
        "  \r\n",
        "    # create dataset\r\n",
        "    train_dataset = CPDataset(\"TOM\",\"cp-vton-plus\")\r\n",
        "\r\n",
        "    # create dataloader\r\n",
        "    train_loader = CPDataLoader(train_dataset)\r\n",
        "\r\n",
        "    # # visualization\r\n",
        "    # if not os.path.exists(opt.tensorboard_dir):\r\n",
        "    #     os.makedirs(opt.tensorboard_dir)\r\n",
        "    # board = SummaryWriter(logdir=os.path.join(opt.tensorboard_dir, opt.name))\r\n",
        "\r\n",
        "    # create model & train & save the final checkpoint\r\n",
        "        # model = UnetGenerator(25, 4, 6, ngf=64, norm_layer=nn.InstanceNorm2d)  # CP-VTON\r\n",
        "    model = UnetGenerator( 26, 4, 6, ngf=64, norm_layer=nn.InstanceNorm2d)  # CP-VTON+\r\n",
        "    train_tom(train_loader, model)\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == \"__main__\":\r\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pzk-8X2RFJQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czX4gkX7Rv9U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}