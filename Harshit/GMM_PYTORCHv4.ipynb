{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GMM_PYTORCH.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWEi4RXgxX3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "397fb042-4f39-478e-c034-5bc7c48c768c"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ypd6IEiKNybZ"
      },
      "source": [
        "# !cp /content/drive/Shareddrives/Virtuon/Pytorch/cp-vton-plus.zip /content/\r\n",
        "# !unzip -qq cp-vton-plus.zip -d /content/"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7W91fgfY4-ce",
        "outputId": "aae26aee-7da9-4756-9d1c-dd154d74a90a"
      },
      "source": [
        "!unzip test.zip"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  test.zip\n",
            "replace test/test_pairs.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "  inflating: test/test/image-mask/demo.png  \n",
            "  inflating: test/test/image-parse-new/demo.png  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiL22rX39fJH",
        "outputId": "6c9ced80-02c5-4131-b284-3080d10f34f5"
      },
      "source": [
        "!zip -r tax.zip /content/test"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/test/ (stored 0%)\n",
            "  adding: content/test/test/ (stored 0%)\n",
            "  adding: content/test/test/image/ (stored 0%)\n",
            "  adding: content/test/test/image/demo.jpeg (deflated 1%)\n",
            "  adding: content/test/test/cloth/ (stored 0%)\n",
            "  adding: content/test/test/cloth/000010_1.jpg (deflated 8%)\n",
            "  adding: content/test/test/warp_mask/ (stored 0%)\n",
            "  adding: content/test/test/warp_mask/demo.jpeg (deflated 23%)\n",
            "  adding: content/test/test/result_dir/ (stored 0%)\n",
            "  adding: content/test/test/result_dir/demo.jpeg (deflated 17%)\n",
            "  adding: content/test/test/pose/ (stored 0%)\n",
            "  adding: content/test/test/pose/demo_keypoints.json (deflated 52%)\n",
            "  adding: content/test/test/image-parse-new/ (stored 0%)\n",
            "  adding: content/test/test/image-parse-new/demo.jpeg (deflated 15%)\n",
            "  adding: content/test/test/image-parse-new/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: content/test/test/image-parse-new/demo.png (deflated 15%)\n",
            "  adding: content/test/test/warp_cloth/ (stored 0%)\n",
            "  adding: content/test/test/warp_cloth/demo.jpeg (deflated 27%)\n",
            "  adding: content/test/test/overlayed_TPS/ (stored 0%)\n",
            "  adding: content/test/test/overlayed_TPS/demo.jpeg (deflated 4%)\n",
            "  adding: content/test/test/warped_grid/ (stored 0%)\n",
            "  adding: content/test/test/image-parse/ (stored 0%)\n",
            "  adding: content/test/test/image-mask/ (stored 0%)\n",
            "  adding: content/test/test/image-mask/demo.jpeg (deflated 13%)\n",
            "  adding: content/test/test/image-mask/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: content/test/test/image-mask/demo.png (deflated 13%)\n",
            "  adding: content/test/test/cloth-mask/ (stored 0%)\n",
            "  adding: content/test/test/cloth-mask/000010_1.jpg (deflated 12%)\n",
            "  adding: content/test/test/cloth-mask/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: content/test/test_pairs.txt (deflated 4%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Lc6xTODFDeo"
      },
      "source": [
        "# !pip install tensorboardX"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qcs8mjOi7NcK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2af9c653-2399-4a41-ca8a-0045397f3cd7"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "import os.path as osp\n",
        "import os\n",
        "import json\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from torch.utils import tensorboard\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 230, in _feed\n",
            "    close()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 177, in close\n",
            "    self._close()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 361, in _close\n",
            "    _close(self._handle)\n",
            "OSError: [Errno 9] Bad file descriptor\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_iyjB38IcVl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1f5eed7-f576-4383-e9a6-3ed25292c25c"
      },
      "source": [
        "class CPDataset(data.Dataset):\n",
        "    def __init__(self, stage='GMM', all_root=\"cp-vton-plus\", data_path = \"data\", mode=\"train\", radius=5, img_height=256, img_width=192):\n",
        "        super(CPDataset, self).__init__()\n",
        "\n",
        "        self.root = all_root\n",
        "\n",
        "        self.data_root = osp.join(all_root,data_path)\n",
        "\n",
        "        self.datamode = mode\n",
        "\n",
        "        self.stage = stage\n",
        "\n",
        "        self.data_list = \"\".join([mode, \"_pairs.txt\"])\n",
        "\n",
        "        self.fine_height = img_height\n",
        "\n",
        "        self.fine_width = img_width\n",
        "\n",
        "        self.radius = radius\n",
        "\n",
        "        self.data_path = osp.join(all_root,data_path, mode)\n",
        "        \n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "        \n",
        "        self.transform_1 = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5), (0.5))\n",
        "        ])\n",
        "\n",
        "        self.transform_2 = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5), (0.5, 0.5))\n",
        "        ])\n",
        "\n",
        "        self.transform_3 = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "        im_names = []\n",
        "        c_names = []\n",
        "\n",
        "        with open(osp.join(self.data_root, self.data_list), 'r') as f:\n",
        "            for line in f.readlines():\n",
        "                im_name, c_name = line.strip().split()\n",
        "                im_names.append(im_name)\n",
        "                c_names.append(c_name)\n",
        "\n",
        "        self.im_names = im_names\n",
        "        self.c_names = c_names\n",
        "\n",
        "    def name(self):\n",
        "        return \"CPDataset\"\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        c_name = self.c_names[index]\n",
        "        im_name = self.im_names[index]\n",
        "        if self.stage == \"GMM\":\n",
        "            c = Image.open(osp.join(self.data_path, 'cloth', c_name))\n",
        "            cm = Image.open(osp.join(self.data_path, 'cloth-mask', c_name)).convert('L')\n",
        "        else:\n",
        "            c = Image.open(osp.join(self.data_path, 'warp-cloth', im_name))\n",
        "            cm = Image.open(osp.join(self.data_path, 'warp-mask', im_name)).convert('L')\n",
        "        \n",
        "        c = self.transform(c)\n",
        "        cm_array = np.array(cm)\n",
        "        cm_array = (cm_array >= 128).astype(np.float32)\n",
        "        cm = torch.from_numpy(cm_array)\n",
        "        cm.unsqueeze_(0)\n",
        "\n",
        "        # person image\n",
        "        im = Image.open(osp.join(self.data_path, 'image', im_name))\n",
        "        im = self.transform(im)\n",
        "\n",
        "        \n",
        "        # LIP labels\n",
        "        \n",
        "        # [(0, 0, 0),    # 0=Background\n",
        "        #  (128, 0, 0),  # 1=Hat\n",
        "        #  (255, 0, 0),  # 2=Hair\n",
        "        #  (0, 85, 0),   # 3=Glove\n",
        "        #  (170, 0, 51),  # 4=SunGlasses\n",
        "        #  (255, 85, 0),  # 5=UpperClothes\n",
        "        #  (0, 0, 85),     # 6=Dress\n",
        "        #  (0, 119, 221),  # 7=Coat\n",
        "        #  (85, 85, 0),    # 8=Socks\n",
        "        #  (0, 85, 85),    # 9=Pants\n",
        "        #  (85, 51, 0),    # 10=Jumpsuits\n",
        "        #  (52, 86, 128),  # 11=Scarf\n",
        "        #  (0, 128, 0),    # 12=Skirt\n",
        "        #  (0, 0, 255),    # 13=Face\n",
        "        #  (51, 170, 221),  # 14=LeftArm\n",
        "        #  (0, 255, 255),   # 15=RightArm\n",
        "        #  (85, 255, 170),  # 16=LeftLeg\n",
        "        #  (170, 255, 85),  # 17=RightLeg\n",
        "        #  (255, 255, 0),   # 18=LeftShoe\n",
        "        #  (255, 170, 0)    # 19=RightShoe\n",
        "        #  (170, 170, 50)   # 20=Skin/Neck/Chest (Newly added after running dataset_neck_skin_correction.py)\n",
        "        #  ]\n",
        "         \n",
        "        # load parsing image\n",
        "        parse_name = im_name.replace('.jpg', '.png')\n",
        "        im_parse = Image.open(osp.join(self.data_path, 'image-parse-new',parse_name)).convert('L')\n",
        "        parse_array = np.array(im_parse)\n",
        "\n",
        "        im_mask = Image.open(osp.join(self.data_path, 'image-mask', parse_name)).convert('L')\n",
        "        mask_array = np.array(im_mask)\n",
        "\n",
        "        parse_shape = (mask_array > 0).astype(np.float32)\n",
        "\n",
        "        if self.stage == 'GMM':\n",
        "            parse_head = (parse_array == 1).astype(np.float32) + (parse_array == 4).astype(np.float32) + (parse_array == 13).astype(np.float32)\n",
        "\n",
        "        else:\n",
        "            parse_head = (parse_array == 1).astype(np.float32) + (parse_array == 2).astype(np.float32) + (parse_array == 4).astype(np.float32) + (parse_array == 9).astype(np.float32) + (parse_array == 12).astype(np.float32) + (parse_array == 13).astype(np.float32) + (parse_array == 16).astype(np.float32) + (parse_array == 17).astype(np.float32)  \n",
        "            \n",
        "        parse_cloth = (parse_array == 5).astype(np.float32) + (parse_array == 6).astype(np.float32) + (parse_array == 7).astype(np.float32)\n",
        "\n",
        "        parse_shape_ori = Image.fromarray((parse_shape*255).astype(np.uint8))\n",
        "\n",
        "        parse_shape = parse_shape_ori.resize((self.fine_width//16, self.fine_height//16), Image.BILINEAR)\n",
        "\n",
        "        parse_shape = parse_shape.resize((self.fine_width, self.fine_height), Image.BILINEAR)\n",
        "        \n",
        "        parse_shape_ori = parse_shape_ori.resize((self.fine_width, self.fine_height), Image.BILINEAR)\n",
        "        \n",
        "        shape_ori = self.transform_1(parse_shape_ori)\n",
        "\n",
        "        shape = self.transform_1(parse_shape)\n",
        "\n",
        "        phead = torch.from_numpy(parse_head)\n",
        "\n",
        "        pcm = torch.from_numpy(parse_cloth)\n",
        "\n",
        "        # Upper Cloth\n",
        "        im_c = im*pcm + (1 - pcm)\n",
        "        im_h = im*phead + (1-phead)\n",
        "\n",
        "        # load pose points\n",
        "        pose_name = im_name.replace('.jpeg', '_keypoints.json')\n",
        "        with open(osp.join(self.data_path, 'pose', pose_name), 'r') as f:\n",
        "            pose_label = json.load(f)\n",
        "            pose_data = pose_label['people'][0]['pose_keypoints']\n",
        "            pose_data = np.array(pose_data)\n",
        "            pose_data = pose_data.reshape([-1,3])\n",
        "        \n",
        "        point_num = pose_data.shape[0]\n",
        "        pose_map = torch.zeros(point_num, self.fine_height, self.fine_width)\n",
        "        \n",
        "        r = self.radius\n",
        "        \n",
        "        im_pose = Image.new('L', (self.fine_width, self.fine_height))\n",
        "        pose_draw = ImageDraw.Draw(im_pose)\n",
        "\n",
        "        for i in range(point_num):\n",
        "            one_map = Image.new('L', (self.fine_width, self.fine_height))\n",
        "            draw = ImageDraw.Draw(one_map)\n",
        "            pointx = pose_data[i, 0]\n",
        "            pointy = pose_data[i, 1]\n",
        "\n",
        "            if pointx > 1 and pointy > 1:\n",
        "                draw.rectangle((pointx - r, pointy - r, pointx + r, pointy + r), 'white', 'white')\n",
        "                pose_draw.rectangle((pointx - r, pointy - r, pointx + r, pointy + r), 'white', 'white')\n",
        "\n",
        "            one_map = self.transform_1(one_map)\n",
        "            pose_map[i] = one_map[0]\n",
        "\n",
        "        im_pose = self.transform_1(im_pose)\n",
        "\n",
        "        agnostic = torch.cat([shape, im_h, pose_map], 0)\n",
        "\n",
        "        # if self.stage == 'GMM':\n",
        "        #     im_g = Image.open(osp.join(self.root, 'grid.png'))\n",
        "        #     im_g = self.transform(im_g)\n",
        "        # else:\n",
        "        #     im_g = ''\n",
        "        \n",
        "        pcm.unsqueeze_(0)\n",
        "        \n",
        "        result = {\n",
        "            'c_name': c_name,\n",
        "            'im_name': im_name,\n",
        "            'cloth': c,\n",
        "            'cloth_mask': cm,\n",
        "            'image': im,\n",
        "            'agnostic': agnostic,\n",
        "            'parse_cloth': im_c,\n",
        "            'shape': shape,\n",
        "            'head': im_h,\n",
        "            'pose_image': im_pose,\n",
        "            # 'grid_image': im_g,\n",
        "            'parse_cloth_mask': pcm,\n",
        "            'shape_ori': shape_ori,\n",
        "        }\n",
        "\n",
        "        return result\n",
        "    def __len__(self):\n",
        "        return len(self.im_names)\n",
        "\n",
        "\n",
        "class CPDataLoader(object):\n",
        "    def __init__(self, dataset, shuffle=True, batch=4, workers=1):\n",
        "        super(CPDataLoader, self).__init__()\n",
        "\n",
        "        if shuffle:\n",
        "            train_sampler = torch.utils.data.sampler.RandomSampler(dataset)\n",
        "        else:\n",
        "            train_sampler = None\n",
        "        \n",
        "        self.data_loader = torch.utils.data.DataLoader(\n",
        "            dataset, batch_size=batch, shuffle=(train_sampler is None),\n",
        "            num_workers=workers, pin_memory=True, sampler=train_sampler\n",
        "        )\n",
        "        self.dataset = dataset\n",
        "        self.data_iter = self.data_loader.__iter__()\n",
        "\n",
        "    def next_batch(self):\n",
        "        try:\n",
        "            batch = self.data_iter.__next__()\n",
        "        except StopIteration:\n",
        "            self.data_iter = self.data_loader.__iter__()\n",
        "            batch = self.data_iter.__next__()\n",
        "        \n",
        "        return batch\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 230, in _feed\n",
            "    close()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 177, in close\n",
            "    self._close()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 361, in _close\n",
            "    _close(self._handle)\n",
            "OSError: [Errno 9] Bad file descriptor\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCnrN2AWEt37"
      },
      "source": [
        "class FeatureL2Norm(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureL2Norm, self).__init__()\n",
        "\n",
        "    def forward(self, feature):\n",
        "        epsilon = 1e-6\n",
        "        norm = torch.pow(torch.sum(torch.pow(feature, 2), 1) +\n",
        "                         epsilon, 0.5).unsqueeze(1).expand_as(feature)\n",
        "        return torch.div(feature, norm)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKZvueIg7ulQ"
      },
      "source": [
        "class FeatureExtraction(nn.Module):\n",
        "    def __init__(self, input_nc, ngf=64, n_layers=3, use_dropout=False):\n",
        "        super(FeatureExtraction, self).__init__()\n",
        "\n",
        "        downconv = nn.Conv2d(input_nc, ngf, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "        model = [downconv, nn.ReLU(True), nn.BatchNorm2d(ngf)]\n",
        "\n",
        "        for i in range(n_layers):\n",
        "            in_ngf = 2**i * ngf if 2**i * ngf < 512 else 512\n",
        "            out_ngf = 2**(i+1) * ngf if 2**i * ngf < 512 else 512\n",
        "            downconv = nn.Conv2d(in_ngf, out_ngf, kernel_size=4, stride=2, padding = 1)\n",
        "            model.append(downconv)\n",
        "            model.append(nn.ReLU(True))\n",
        "            model.append(nn.BatchNorm2d(out_ngf))\n",
        "        \n",
        "        model.append(nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1))\n",
        "        model.append(nn.ReLU(True))\n",
        "        model.append(nn.BatchNorm2d(512))\n",
        "        model.append(nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1))\n",
        "        model.append(nn.ReLU(True))\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "\n",
        "class FeatureCorrelation(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureCorrelation, self).__init__()\n",
        "    def forward(self, feature_A, feature_B):\n",
        "        b,c,h,w = feature_A.size()\n",
        "        \n",
        "        feature_A = feature_A.transpose(2,3).contiguous().view(b, c, h*w)\n",
        "        feature_B = feature_B.contiguous().view(b, c, h*w).transpose(1,2)\n",
        "\n",
        "        feature_mul = torch.bmm(feature_B, feature_A)\n",
        "        correlation_tensor = feature_mul.view(b, h, w, h*w).transpose(2,3).transpose(1,2)\n",
        "\n",
        "        return correlation_tensor\n",
        "        # return feature_mul\n",
        "\n",
        "class FeatureRegression(nn.Module):\n",
        "    def __init__(self, input_nc=512, output_dim=50, use_cuda=True):\n",
        "        super(FeatureRegression, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_nc, 512, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        self.linear = nn.Linear(64 * 4 * 3, output_dim)\n",
        "        self.tanh = nn.Tanh()\n",
        "        if use_cuda:\n",
        "            self.conv.cuda()\n",
        "            self.tanh.cuda()\n",
        "            self.linear.cuda()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        # x = x.view(x.size(0), -1)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.linear(x)\n",
        "        x = self.tanh(x)\n",
        "        return x"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yvPoHGcQecb"
      },
      "source": [
        "class TpsGridGen(nn.Module):\n",
        "    def __init__(self, out_h=256, out_w=192, use_regular_grid=True, grid_size=5, reg_factor=0, use_cuda=True):\n",
        "        super(TpsGridGen, self).__init__()\n",
        "        self.out_h, self.out_w = out_h, out_w\n",
        "        self.reg_factor = reg_factor\n",
        "        self.use_cuda = use_cuda\n",
        "\n",
        "        # create grid in numpy\n",
        "        self.grid = np.zeros([self.out_h, self.out_w, 3], dtype=np.float32)\n",
        "        # sampling grid with dim-0 coords (Y)\n",
        "        self.grid_X, self.grid_Y = np.meshgrid(\n",
        "            np.linspace(-1, 1, out_w), np.linspace(-1, 1, out_h))\n",
        "        # grid_X,grid_Y: size [1,H,W,1,1]\n",
        "        self.grid_X = torch.FloatTensor(self.grid_X).unsqueeze(0).unsqueeze(3)\n",
        "        self.grid_Y = torch.FloatTensor(self.grid_Y).unsqueeze(0).unsqueeze(3)\n",
        "        if use_cuda:\n",
        "            self.grid_X = self.grid_X.cuda()\n",
        "            self.grid_Y = self.grid_Y.cuda()\n",
        "\n",
        "        # initialize regular grid for control points P_i\n",
        "        if use_regular_grid:\n",
        "            axis_coords = np.linspace(-1, 1, grid_size)\n",
        "            self.N = grid_size*grid_size\n",
        "            P_Y, P_X = np.meshgrid(axis_coords, axis_coords)\n",
        "            P_X = np.reshape(P_X, (-1, 1))  # size (N,1)\n",
        "            P_Y = np.reshape(P_Y, (-1, 1))  # size (N,1)\n",
        "            P_X = torch.FloatTensor(P_X)\n",
        "            P_Y = torch.FloatTensor(P_Y)\n",
        "            self.P_X_base = P_X.clone()\n",
        "            self.P_Y_base = P_Y.clone()\n",
        "            self.Li = self.compute_L_inverse(P_X, P_Y).unsqueeze(0)\n",
        "            self.P_X = P_X.unsqueeze(2).unsqueeze(\n",
        "                3).unsqueeze(4).transpose(0, 4)\n",
        "            self.P_Y = P_Y.unsqueeze(2).unsqueeze(\n",
        "                3).unsqueeze(4).transpose(0, 4)\n",
        "            if use_cuda:\n",
        "                self.P_X = self.P_X.cuda()\n",
        "                self.P_Y = self.P_Y.cuda()\n",
        "                self.P_X_base = self.P_X_base.cuda()\n",
        "                self.P_Y_base = self.P_Y_base.cuda()\n",
        "\n",
        "    def forward(self, theta):\n",
        "        warped_grid = self.apply_transformation(\n",
        "            theta, torch.cat((self.grid_X, self.grid_Y), 3))\n",
        "\n",
        "        return warped_grid\n",
        "\n",
        "    def compute_L_inverse(self, X, Y):\n",
        "        N = X.size()[0]  # num of points (along dim 0)\n",
        "        # construct matrix K\n",
        "        Xmat = X.expand(N, N)\n",
        "        Ymat = Y.expand(N, N)\n",
        "        P_dist_squared = torch.pow(\n",
        "            Xmat-Xmat.transpose(0, 1), 2)+torch.pow(Ymat-Ymat.transpose(0, 1), 2)\n",
        "        # make diagonal 1 to avoid NaN in log computation\n",
        "        P_dist_squared[P_dist_squared == 0] = 1\n",
        "        K = torch.mul(P_dist_squared, torch.log(P_dist_squared))\n",
        "        # construct matrix L\n",
        "        O = torch.FloatTensor(N, 1).fill_(1)\n",
        "        Z = torch.FloatTensor(3, 3).fill_(0)\n",
        "        P = torch.cat((O, X, Y), 1)\n",
        "        L = torch.cat((torch.cat((K, P), 1), torch.cat(\n",
        "            (P.transpose(0, 1), Z), 1)), 0)\n",
        "        Li = torch.inverse(L)\n",
        "        if self.use_cuda:\n",
        "            Li = Li.cuda()\n",
        "        return Li\n",
        "\n",
        "    def apply_transformation(self, theta, points):\n",
        "        if theta.dim() == 2:\n",
        "            theta = theta.unsqueeze(2).unsqueeze(3)\n",
        "        # points should be in the [B,H,W,2] format,\n",
        "        # where points[:,:,:,0] are the X coords\n",
        "        # and points[:,:,:,1] are the Y coords\n",
        "\n",
        "        # input are the corresponding control points P_i\n",
        "        batch_size = theta.size()[0]\n",
        "        # split theta into point coordinates\n",
        "        Q_X = theta[:, :self.N, :, :].squeeze(3)\n",
        "        Q_Y = theta[:, self.N:, :, :].squeeze(3)\n",
        "        Q_X = Q_X + self.P_X_base.expand_as(Q_X)\n",
        "        Q_Y = Q_Y + self.P_Y_base.expand_as(Q_Y)\n",
        "\n",
        "        # get spatial dimensions of points\n",
        "        points_b = points.size()[0]\n",
        "        points_h = points.size()[1]\n",
        "        points_w = points.size()[2]\n",
        "\n",
        "        # repeat pre-defined control points along spatial dimensions of points to be transformed\n",
        "        P_X = self.P_X.expand((1, points_h, points_w, 1, self.N))\n",
        "        P_Y = self.P_Y.expand((1, points_h, points_w, 1, self.N))\n",
        "\n",
        "        # compute weigths for non-linear part\n",
        "        W_X = torch.bmm(self.Li[:, :self.N, :self.N].expand(\n",
        "            (batch_size, self.N, self.N)), Q_X)\n",
        "        W_Y = torch.bmm(self.Li[:, :self.N, :self.N].expand(\n",
        "            (batch_size, self.N, self.N)), Q_Y)\n",
        "        # reshape\n",
        "        # W_X,W,Y: size [B,H,W,1,N]\n",
        "        W_X = W_X.unsqueeze(3).unsqueeze(4).transpose(\n",
        "            1, 4).repeat(1, points_h, points_w, 1, 1)\n",
        "        W_Y = W_Y.unsqueeze(3).unsqueeze(4).transpose(\n",
        "            1, 4).repeat(1, points_h, points_w, 1, 1)\n",
        "        # compute weights for affine part\n",
        "        A_X = torch.bmm(self.Li[:, self.N:, :self.N].expand(\n",
        "            (batch_size, 3, self.N)), Q_X)\n",
        "        A_Y = torch.bmm(self.Li[:, self.N:, :self.N].expand(\n",
        "            (batch_size, 3, self.N)), Q_Y)\n",
        "        # reshape\n",
        "        # A_X,A,Y: size [B,H,W,1,3]\n",
        "        A_X = A_X.unsqueeze(3).unsqueeze(4).transpose(\n",
        "            1, 4).repeat(1, points_h, points_w, 1, 1)\n",
        "        A_Y = A_Y.unsqueeze(3).unsqueeze(4).transpose(\n",
        "            1, 4).repeat(1, points_h, points_w, 1, 1)\n",
        "\n",
        "        # compute distance P_i - (grid_X,grid_Y)\n",
        "        # grid is expanded in point dim 4, but not in batch dim 0, as points P_X,P_Y are fixed for all batch\n",
        "        points_X_for_summation = points[:, :, :, 0].unsqueeze(\n",
        "            3).unsqueeze(4).expand(points[:, :, :, 0].size()+(1, self.N))\n",
        "        points_Y_for_summation = points[:, :, :, 1].unsqueeze(\n",
        "            3).unsqueeze(4).expand(points[:, :, :, 1].size()+(1, self.N))\n",
        "\n",
        "        if points_b == 1:\n",
        "            delta_X = points_X_for_summation-P_X\n",
        "            delta_Y = points_Y_for_summation-P_Y\n",
        "        else:\n",
        "            # use expanded P_X,P_Y in batch dimension\n",
        "            delta_X = points_X_for_summation - \\\n",
        "                P_X.expand_as(points_X_for_summation)\n",
        "            delta_Y = points_Y_for_summation - \\\n",
        "                P_Y.expand_as(points_Y_for_summation)\n",
        "\n",
        "        dist_squared = torch.pow(delta_X, 2)+torch.pow(delta_Y, 2)\n",
        "        # U: size [1,H,W,1,N]\n",
        "        dist_squared[dist_squared == 0] = 1  # avoid NaN in log computation\n",
        "        U = torch.mul(dist_squared, torch.log(dist_squared))\n",
        "\n",
        "        # expand grid in batch dimension if necessary\n",
        "        points_X_batch = points[:, :, :, 0].unsqueeze(3)\n",
        "        points_Y_batch = points[:, :, :, 1].unsqueeze(3)\n",
        "        if points_b == 1:\n",
        "            points_X_batch = points_X_batch.expand(\n",
        "                (batch_size,)+points_X_batch.size()[1:])\n",
        "            points_Y_batch = points_Y_batch.expand(\n",
        "                (batch_size,)+points_Y_batch.size()[1:])\n",
        "\n",
        "        points_X_prime = A_X[:, :, :, :, 0] + \\\n",
        "            torch.mul(A_X[:, :, :, :, 1], points_X_batch) + \\\n",
        "            torch.mul(A_X[:, :, :, :, 2], points_Y_batch) + \\\n",
        "            torch.sum(torch.mul(W_X, U.expand_as(W_X)), 4)\n",
        "\n",
        "        points_Y_prime = A_Y[:, :, :, :, 0] + \\\n",
        "            torch.mul(A_Y[:, :, :, :, 1], points_X_batch) + \\\n",
        "            torch.mul(A_Y[:, :, :, :, 2], points_Y_batch) + \\\n",
        "            torch.sum(torch.mul(W_Y, U.expand_as(W_Y)), 4)\n",
        "\n",
        "        return torch.cat((points_X_prime, points_Y_prime), 3)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Al9XfdZCI4a"
      },
      "source": [
        "class GMM(nn.Module):\n",
        "    def __init__(self, grid_size = 5, fine_height=256, fine_width=192):\n",
        "        super(GMM, self).__init__()\n",
        "        self.extractionA = FeatureExtraction(22, ngf=64, n_layers=3)\n",
        "        self.extractionB = FeatureExtraction(1, ngf=64, n_layers=3)\n",
        "        self.l2norm = FeatureL2Norm()\n",
        "        self.correlation = FeatureCorrelation()\n",
        "        self.regression = FeatureRegression(input_nc=192, output_dim=2*grid_size**2, use_cuda=True)\n",
        "        self.gridGen = TpsGridGen(fine_height, fine_width, use_cuda=True, grid_size=grid_size)\n",
        "\n",
        "    def forward(self, inputA, inputB):\n",
        "        featureA = self.extractionA(inputA)\n",
        "        featureB = self.extractionB(inputB)\n",
        "        featureA = self.l2norm(featureA)\n",
        "        featureB = self.l2norm(featureB)\n",
        "        correlation = self.correlation(featureA.cuda(), featureB.cuda())\n",
        "\n",
        "        theta = self.regression(correlation)\n",
        "        grid = self.gridGen(theta)\n",
        "        return grid, theta\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrOYOp-ZZHgV"
      },
      "source": [
        "class DT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DT, self).__init__()\n",
        "    \n",
        "    def forward(self, x1, x2):\n",
        "        dt = torch.abs(x1 - x2)\n",
        "        return dt\n",
        "\n",
        "class DT2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DT2, self).__init__()\n",
        "\n",
        "    def forward(self, x1, y1, x2, y2):\n",
        "        dt = torch.sqrt(torch.mul(x1 - x2, x1 - x2) + torch.mul(y1 - y2, y1 - y2))\n",
        "        return dt"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRDxtQO8Y0er"
      },
      "source": [
        "class GicLoss(nn.Module):\n",
        "    def __init__(self, fine_height=256, fine_width=192):\n",
        "        super(GicLoss, self).__init__()\n",
        "\n",
        "        self.dT = DT()\n",
        "\n",
        "        self.fine_height = fine_height\n",
        "        self.fine_width = fine_width\n",
        "\n",
        "    def forward(self, grid):\n",
        "        Gx = grid[:,:,:,0]\n",
        "        Gy = grid[:,:,:,1]\n",
        "        \n",
        "        Gxcenter = Gx[:, 1:self.fine_height - 1, 1:self.fine_width - 1]\n",
        "        # Gxup = Gx[:, 0:self.fine_height - 2, 1:self.fine_width - 1]\n",
        "        # Gxdown = Gx[:, 2:self.fine_height, 1:self.fine_width - 1]\n",
        "        Gxleft = Gx[:, 1:self.fine_height - 1, 0:self.fine_width - 2]\n",
        "        Gxright = Gx[:, 1:self.fine_height - 1, 2:self.fine_width]\n",
        "\n",
        "        Gycenter = Gy[:, 1:self.fine_height - 1, 1:self.fine_width - 1]\n",
        "        Gyup = Gy[:, 0:self.fine_height - 2, 1:self.fine_width - 1]\n",
        "        Gydown = Gy[:, 2:self.fine_height, 1:self.fine_width - 1]\n",
        "        # Gyleft = Gy[:, 1:self.fine_height - 1, 0:self.fine_width - 2]\n",
        "        # Gyright = Gy[:, 1:self.fine_height - 1, 2:self.fine_width]\n",
        "\n",
        "        dtleft = self.dT(Gxleft, Gxcenter)\n",
        "        dtright = self.dT(Gxright, Gxcenter)\n",
        "        dtup = self.dT(Gyup, Gycenter)\n",
        "        dtdown = self.dT(Gydown, Gycenter)\n",
        "\n",
        "        return torch.sum(torch.abs(dtleft - dtright) + torch.abs(dtup - dtdown))"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bbb6SquvdEe"
      },
      "source": [
        "def save_checkpoint(model, save_path):\n",
        "    if not os.path.exists(os.path.dirname(save_path)):\n",
        "        os.makedirs(os.path.dirname(save_path))\n",
        "\n",
        "    torch.save(model.cpu().state_dict(), save_path)\n",
        "    model.cuda()\n",
        "\n",
        "\n",
        "def load_checkpoint(model, checkpoint_path):\n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        return\n",
        "    model.load_state_dict(torch.load(checkpoint_path))\n",
        "    model.cuda()"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ie7ZJxWEEbus"
      },
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "def tensor_for_board(img_tensor):\n",
        "    # map into [0,1]\n",
        "    tensor = (img_tensor.clone()+1) * 0.5\n",
        "    tensor.cpu().clamp(0, 1)\n",
        "\n",
        "    if tensor.size(1) == 1:\n",
        "        tensor = tensor.repeat(1, 3, 1, 1)\n",
        "\n",
        "    return tensor\n",
        "\n",
        "\n",
        "def tensor_list_for_board(img_tensors_list):\n",
        "    grid_h = len(img_tensors_list)\n",
        "    grid_w = max(len(img_tensors) for img_tensors in img_tensors_list)\n",
        "\n",
        "    batch_size, channel, height, width = tensor_for_board(\n",
        "        img_tensors_list[0][0]).size()\n",
        "    canvas_h = grid_h * height\n",
        "    canvas_w = grid_w * width\n",
        "    canvas = torch.FloatTensor(\n",
        "        batch_size, channel, canvas_h, canvas_w).fill_(0.5)\n",
        "    for i, img_tensors in enumerate(img_tensors_list):\n",
        "        for j, img_tensor in enumerate(img_tensors):\n",
        "            offset_h = i * height\n",
        "            offset_w = j * width\n",
        "            tensor = tensor_for_board(img_tensor)\n",
        "            canvas[:, :, offset_h: offset_h + height,\n",
        "                   offset_w: offset_w + width].copy_(tensor)\n",
        "\n",
        "    return canvas\n",
        "\n",
        "\n",
        "def board_add_image(board, tag_name, img_tensor, step_count):\n",
        "    tensor = tensor_for_board(img_tensor)\n",
        "\n",
        "    for i, img in enumerate(tensor):\n",
        "        board.add_image('%s/%03d' % (tag_name, i), img, step_count)\n",
        "\n",
        "\n",
        "def board_add_images(board, tag_name, img_tensors_list, step_count):\n",
        "    tensor = tensor_list_for_board(img_tensors_list)\n",
        "\n",
        "    for i, img in enumerate(tensor):\n",
        "        board.add_image('%s/%03d' % (tag_name, i), img, step_count)\n",
        "\n",
        "\n",
        "def save_images(img_tensors, img_names, save_dir):\n",
        "    for img_tensor, img_name in zip(img_tensors, img_names):\n",
        "        tensor = (img_tensor.clone()+1)*0.5 * 255\n",
        "        tensor = tensor.cpu().clamp(0, 255)\n",
        "\n",
        "        array = tensor.detach().numpy().astype('uint8')\n",
        "        if array.shape[0] == 1:\n",
        "            array = array.squeeze(0)\n",
        "        elif array.shape[0] == 3:\n",
        "            array = array.swapaxes(0, 1).swapaxes(1, 2)\n",
        "\n",
        "        Image.fromarray(array).save(os.path.join(save_dir, img_name))"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBkUJDjgFlsN"
      },
      "source": [
        "def train_GMM(model, train_loader, board,lr = 1e-4, keep_step = 100000, decay_step = 100000, save_count=500, display_count = 100, checkpoint_dir=\"/content/checkpoint\", name = 'GMM'):\n",
        "    model.cuda()\n",
        "    model.train()\n",
        "\n",
        "    L1loss = nn.L1Loss()\n",
        "    Gicloss = GicLoss()\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer,\n",
        "                                                  lr_lambda= lambda step: 1.0 - (max(0, step - keep_step)/float(decay_step + 1)))\n",
        "    \n",
        "    for step in range(keep_step + decay_step):\n",
        "        iter_start_time = time.time()\n",
        "        inputs = train_loader.next_batch()\n",
        "\n",
        "        im = inputs['image'].cuda() #full image of person\n",
        "        im_pose = inputs['pose_image'].cuda() # pose channels\n",
        "        im_h = inputs['head'].cuda()     # person head Image \n",
        "        shape = inputs['shape'].cuda()    # blurred binary mask of person and \n",
        "        agnostic = inputs['agnostic'].cuda()  # person Representation for GMM\n",
        "        c = inputs['cloth'].cuda()        # in shop cloths\n",
        "        cm = inputs['cloth_mask'].cuda()    # in shop cloth mask\n",
        "        im_c = inputs['parse_cloth'].cuda() # GT for GMM\n",
        "        im_g = inputs['grid_image'].cuda()  # grid image for Viz.\n",
        "        pcm = inputs['parse_cloth_mask'].cuda()\n",
        "\n",
        "        grid, theta = model(agnostic, cm)\n",
        "        warped_cloth = F.grid_sample(c, grid, padding_mode='border', align_corners=True)\n",
        "        warped_mask = F.grid_sample(cm, grid, padding_mode='zeros', align_corners=True)\n",
        "        warped_grid = F.grid_sample(im_g, grid, padding_mode='zeros', align_corners=True)\n",
        "\n",
        "        visuals = [[im_h, shape, im_pose],\n",
        "                   [c, warped_cloth, im_c],\n",
        "                   [warped_grid, (warped_cloth+im)*0.5, im]]\n",
        "        \n",
        "        Lwarp = L1loss(warped_mask, pcm)\n",
        "\n",
        "        Lgic = Gicloss(grid)\n",
        "\n",
        "        Lgic = Lgic / (grid.shape[0] * grid.shape[1] * grid.shape[2])\n",
        "\n",
        "        loss = Lwarp + 40 * Lgic\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (step+1) % display_count == 0:\n",
        "            board_add_images(board, 'combine', visuals, step+1)\n",
        "            board.add_scalar('loss', loss.item(), step+1)\n",
        "            board.add_scalar('40*Lgic', (40*Lgic).item(), step+1)\n",
        "            board.add_scalar('Lwarp', Lwarp.item(), step+1)\n",
        "            t = time.time() - iter_start_time\n",
        "            print('step: %8d, time: %.3f, loss: %4f, (40*Lgic): %.8f, Lwarp: %.6f' %\n",
        "                  (step+1, t, loss.item(), (40*Lgic).item(), Lwarp.item()), flush=True)\n",
        "        if (step+1) % save_count == 0:\n",
        "            save_checkpoint(model, os.path.join(\n",
        "                checkpoint_dir, name, 'step_%06d.pth' % (step+1)))"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65h756QJh-a7"
      },
      "source": [
        "def dir(path, name):\r\n",
        "    name = osp.join(path, name)\r\n",
        "    if not osp.exists(name):\r\n",
        "        os.makedirs(name)\r\n",
        "    return name"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHgbVZJQXbFZ"
      },
      "source": [
        "def test_GMM(model, test_loader, checkpoint_path = '/content/drive/Shareddrives/Virtuon/Working GMM/', name = \"GMM\", model_name = \"PreTrainedGMM\", result_dir = \"results\", mode = 'train'):\r\n",
        "\r\n",
        "    model_path = osp.join(checkpoint_path, name, model_name + \".pth\")\r\n",
        "    load_checkpoint(model, model_path)\r\n",
        "\r\n",
        "    model.cuda()\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    save_dir = osp.join(result_dir, name, mode)\r\n",
        "\r\n",
        "    if not osp.exists(save_dir):\r\n",
        "        os.makedirs(save_dir)\r\n",
        "\r\n",
        "    warp_cloth_dir = dir(save_dir, 'warp_cloth')\r\n",
        "\r\n",
        "    warp_mask_dir = dir(save_dir, 'warp_mask')\r\n",
        "\r\n",
        "    result_dir = dir(save_dir, 'result_dir')\r\n",
        "\r\n",
        "    overlayed_TPS_dir = dir(save_dir, 'overlayed_TPS')\r\n",
        "\r\n",
        "    warped_grid_dir = dir(save_dir, 'warped_grid')\r\n",
        "\r\n",
        "    for step, inputs in enumerate(test_loader.data_loader):\r\n",
        "        iter_start_time = time.time()\r\n",
        "\r\n",
        "        c_names = inputs['c_name']\r\n",
        "        im_names = inputs['im_name']\r\n",
        "        im = inputs['image'].cuda()\r\n",
        "        im_pose = inputs['pose_image'].cuda()\r\n",
        "        im_h = inputs['head'].cuda()\r\n",
        "        shape = inputs['shape'].cuda()\r\n",
        "        agnostic = inputs['agnostic'].cuda()\r\n",
        "        c = inputs['cloth'].cuda()\r\n",
        "        cm = inputs['cloth_mask'].cuda()\r\n",
        "        im_c = inputs['parse_cloth'].cuda()\r\n",
        "        # im_g = inputs['grid_image'].cuda()\r\n",
        "        shape_ori = inputs['shape_ori']  # original body shape without blurring\r\n",
        "\r\n",
        "        grid, theta = model(agnostic, cm)\r\n",
        "        warped_cloth = F.grid_sample(c, grid, padding_mode='border', align_corners=True)\r\n",
        "        warped_mask = F.grid_sample(cm, grid, padding_mode='zeros', align_corners=True)\r\n",
        "        # warped_grid = F.grid_sample(im_g, grid, padding_mode='zeros', align_corners=True)\r\n",
        "        overlay = 0.7 * warped_cloth + 0.3 * im\r\n",
        "\r\n",
        "        # visuals = [[im_h, shape, im_pose],\r\n",
        "        #            [c, warped_cloth, im_c],\r\n",
        "        #            [warped_grid, (warped_cloth+im)*0.5, im]]\r\n",
        "\r\n",
        "        # save_images(warped_cloth, c_names, warp_cloth_dir)\r\n",
        "        # save_images(warped_mask*2-1, c_names, warp_mask_dir)\r\n",
        "        save_images(warped_cloth, im_names, warp_cloth_dir)\r\n",
        "        save_images(warped_mask * 2 - 1, im_names, warp_mask_dir)\r\n",
        "        save_images(shape_ori.cuda() * 0.2 + warped_cloth *\r\n",
        "                    0.8, im_names, result_dir)\r\n",
        "        # save_images(warped_grid, im_names, warped_grid_dir)\r\n",
        "        save_images(overlay, im_names, overlayed_TPS_dir)\r\n",
        "\r\n",
        "        if (step+1) % 100 == 0:\r\n",
        "        #     board_add_images(board, 'combine', visuals, step+1)\r\n",
        "            t = time.time() - iter_start_time\r\n",
        "            print('step: %8d, time: %.3f' % (step+1, t), flush=True)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2fStQEoxRUk"
      },
      "source": [
        "# from torch.utils.tensorboard.writer import SummaryWriter\r\n",
        "\r\n",
        "# tensorboard_dir = '/content/tensorboard'\r\n",
        "# name = 'GMM_2'\r\n",
        "model = GMM()\r\n",
        "train_dataset = CPDataset(mode='test', all_root='test', data_path='', )\r\n",
        "train_loader = CPDataLoader(train_dataset, batch=1)\r\n",
        "\r\n",
        "# for viz.\r\n",
        "# if not os.path.exists(tensorboard_dir):\r\n",
        "#         os.makedirs(os.path.join(tensorboard_dir, name))\r\n",
        "# board = SummaryWriter(log_dir=os.path.join(tensorboard_dir, name))\r\n",
        "\r\n",
        "\r\n",
        "# %load_ext tensorboard\r\n",
        "# %tensorboard --logdir /content/tensorboard/GMM_2\r\n"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ag3Ou2Ec6DEo"
      },
      "source": [
        "#train_GMM(model, train_loader,board,save_count=1000,display_count=100, checkpoint_dir = '/content/drive/Shareddrives/Virtuon/Working GMM/')"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqKsVR_MIrpK"
      },
      "source": [
        "test_GMM(model, train_loader, mode='test')"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqfEhlrPOzMt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab54a417-f0f1-4679-9936-193fcf9dd383"
      },
      "source": [
        "!zip -r result.zip /content/results/GMM/train "
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tzip warning: name not matched: /content/results/GMM/train\n",
            "\n",
            "zip error: Nothing to do! (try: zip -r result.zip . -i /content/results/GMM/train)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yza5kADr5OI9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7a3d23d-025f-42a5-b550-4948c157fc74"
      },
      "source": [
        "!cp result.zip /content/drive/Shareddrives/Virtuon/Pytorch"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot stat 'result.zip': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSRWQJ8_6Jpe"
      },
      "source": [
        ""
      ],
      "execution_count": 64,
      "outputs": []
    }
  ]
}